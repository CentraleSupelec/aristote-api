{
    "original_file_name": "RI2.mp4",
    "language": "fr",
    "sentences": [
        {
            "start": 0,
            "end": 7,
            "text": "Bonjour, on attaque aujourd'hui la deuxi\u00e8me partie du cours d'apprentissage pour la recherche d'information."
        },
        {
            "start": 7,
            "end": 14,
            "text": "Je vais rapidement commencer par un r\u00eave r\u00e9capitulatif des notions introduites la derni\u00e8re fois."
        },
        {
            "start": 14,
            "end": 49,
            "text": "La derni\u00e8re fois, j'ai pr\u00e9sent\u00e9 un nouveau paradigme pour la recherche d'information qui est un paradigme bas\u00e9 sur de l'apprentissage qui s'appelle \u00ab Learning to Rank \u00bb o\u00f9 on a un jeu de donn\u00e9es d'apprentissage qui est compos\u00e9 principalement de paires requ\u00eates, documents, d\u00e9crits caract\u00e9ris\u00e9s avec un vecteur caract\u00e9ristique qu'il faut construire et puis des donn\u00e9es de sortie, des labels qui sont des valeurs de pertinence."
        },
        {
            "start": 49,
            "end": 70,
            "text": "A partir de ces donn\u00e9es d'apprentissage, on apprend une fonction qui va \u00eatre une fonction d'ordonnancement ou un crit\u00e8re et puis cette fonction d'ordonnancement apprise, on l'utilise ensuite sur des donn\u00e9es de test ou des donn\u00e9es de production qui consistent uniquement en des donn\u00e9es non labellis\u00e9es."
        },
        {
            "start": 70,
            "end": 73,
            "text": "Bien s\u00fbr, on n'a pas la v\u00e9rit\u00e9 de terrain pour ces donn\u00e9es."
        },
        {
            "start": 73,
            "end": 83,
            "text": "Et puis sur ces donn\u00e9es de test, on utilise la fonction F apprise pour pr\u00e9dire la pertinence et classer les r\u00e9sultats."
        },
        {
            "start": 83,
            "end": 86,
            "text": "En sortie, on obtient bien toujours une liste ordonn\u00e9e de documents."
        },
        {
            "start": 86,
            "end": 100,
            "text": "Ce principe d'apprentissage pour l'AERI repose sur deux concepts principaux qui sont la notation des donn\u00e9es et la repr\u00e9sentation des couples requ\u00eates-documents dans l'espace des caract\u00e9ristiques."
        },
        {
            "start": 100,
            "end": 105,
            "text": "C'est vraiment une \u00e9tape tr\u00e8s importante qu'on appelle l'\u00e9tape de description."
        },
        {
            "start": 105,
            "end": 112,
            "text": "Et puis la partie apprentissage en tant que telle avec une structure d'apprentissage classique en deux \u00e9tapes."
        },
        {
            "start": 112,
            "end": 118,
            "text": "Une \u00e9tape d'apprentissage de la fonction d'ordonnancement F, puis l'\u00e9tape de test de production sur de nouvelles requ\u00eates."
        },
        {
            "start": 118,
            "end": 131,
            "text": "On peut bien s\u00fbr aussi rajouter \u00e0 \u00e7a une \u00e9tape de validation qui, par exemple, pourrait consister \u00e0 choisir les hyperparam\u00e8tres du mod\u00e8le."
        },
        {
            "start": 131,
            "end": 141,
            "text": "La derni\u00e8re fois, il avait \u00e9t\u00e9 aussi introduit le fait qu'il existait trois grandes familles d'approches dans ce paradigm learning to rank."
        },
        {
            "start": 141,
            "end": 148,
            "text": "L'approche par points, dont on a parl\u00e9 la derni\u00e8re fois, o\u00f9 les documents sont consid\u00e9r\u00e9s de mani\u00e8re ind\u00e9pendante."
        },
        {
            "start": 148,
            "end": 153,
            "text": "L'approche par paire, dont on va parler aujourd'hui."
        },
        {
            "start": 153,
            "end": 158,
            "text": "Et l'approche par liste, qui va \u00eatre aussi rapidement \u00e9voqu\u00e9e aujourd'hui."
        },
        {
            "start": 158,
            "end": 169,
            "text": "L'approche par points, c'est une approche qui consiste \u00e0 transformer le probl\u00e8me de classement en un probl\u00e8me soit de classification, soit de r\u00e9gression, soit de r\u00e9gression ordinale."
        },
        {
            "start": 169,
            "end": 175,
            "text": "Selon le type des valeurs de pertinence qu'on a \u00e0 disposition."
        },
        {
            "start": 175,
            "end": 183,
            "text": "Typiquement, si on a des valeurs de pertinence qui sont des nombres r\u00e9els, on va se ramener \u00e0 un probl\u00e8me de r\u00e9gression."
        },
        {
            "start": 183,
            "end": 187,
            "text": "Si on a des valeurs de pertinence qui sont des cat\u00e9gories, on va se ramener \u00e0 un probl\u00e8me de classification."
        },
        {
            "start": 187,
            "end": 192,
            "text": "Et enfin, si on a des cat\u00e9gories ordonn\u00e9es, on va se ramener \u00e0 un probl\u00e8me de r\u00e9gression ordinaire."
        },
        {
            "start": 192,
            "end": 199,
            "text": "Donc voil\u00e0, on a notre vecteur caract\u00e9ristique."
        },
        {
            "start": 199,
            "end": 203,
            "text": "Trois types de probl\u00e8mes selon le type de valeur de sortie."
        },
        {
            "start": 203,
            "end": 210,
            "text": "On en d\u00e9duit la fonction f2x, on apprend la fonction f2x, qui va \u00eatre une fonction de classement."
        },
        {
            "start": 210,
            "end": 212,
            "text": "\u00c7a c'est la phase d'apprentissage."
        },
        {
            "start": 212,
            "end": 220,
            "text": "Et pour la phase de classement en tant que telle, on a nos donn\u00e9es qui sont d\u00e9crites selon le m\u00eame sch\u00e9ma de description."
        },
        {
            "start": 220,
            "end": 253,
            "text": "Et puis on va ordonner les vecteurs, les donn\u00e9es, selon le score de cette fonction f. On va aujourd'hui introduire l'approche par paire, qui est une approche tr\u00e8s d\u00e9velopp\u00e9e, tr\u00e8s utilis\u00e9e dans le paradigme learning to rank, notamment parce qu'on va pouvoir facilement cr\u00e9er des jeux de donn\u00e9es d'apprentissage, notamment pour les collections de type web."
        },
        {
            "start": 253,
            "end": 261,
            "text": "L'id\u00e9e principale de l'approche par paire, c'est de travailler sur des paires de pr\u00e9f\u00e9rences."
        },
        {
            "start": 261,
            "end": 268,
            "text": "Cette id\u00e9e, elle se base sur le fait que la notion de pertinence n'est pas une notion absolue."
        },
        {
            "start": 268,
            "end": 282,
            "text": "Plusieurs fois, tout au long du cours, on a \u00e9voqu\u00e9 cette difficult\u00e9 \u00e0 caract\u00e9riser correctement, \u00e0 d\u00e9finir correctement cette notion de pertinence, avec notamment d'ailleurs des d\u00e9finitions qui sont tr\u00e8s d\u00e9pendantes des mod\u00e8les de recherche."
        },
        {
            "start": 282,
            "end": 291,
            "text": "Binaires pour le mod\u00e8le bool\u00e9en, probabilistes pour le mod\u00e8le probabiliste, etc."
        },
        {
            "start": 291,
            "end": 313,
            "text": "Quand on a parl\u00e9 de la partie \u00e9valuation des mod\u00e8les de recherche, on a aussi \u00e9voqu\u00e9 le fait qu'il \u00e9tait difficile de juger de la pertinence d'un document, mais par contre qu'il \u00e9tait plus facile de juger de la pertinence relative de deux documents."
        },
        {
            "start": 313,
            "end": 336,
            "text": "\u00c9tant donn\u00e9 le r\u00e9sultat d'une recherche, la r\u00e9ponse d'un moteur de recherche \u00e0 un besoin d'information, pour moi utilisateur, il va \u00eatre plus facile de dire si le document N qui m'a \u00e9t\u00e9 retourn\u00e9 est plus pertinent que le document N plus 1, que de juger de la pertinence de chacun des documents."
        },
        {
            "start": 336,
            "end": 342,
            "text": "C'est vraiment cette id\u00e9e-l\u00e0 qui va \u00eatre mise en \u0153uvre dans l'approche par paire."
        },
        {
            "start": 342,
            "end": 349,
            "text": "On va travailler avec ce qu'on va appeler des jugements par pr\u00e9f\u00e9rence, et donc avec la notion de paire de pr\u00e9f\u00e9rence."
        },
        {
            "start": 349,
            "end": 358,
            "text": "Typiquement, ces paires de pr\u00e9f\u00e9rence vont exprimer le fait qu'un document va \u00eatre pr\u00e9f\u00e9r\u00e9 \u00e0 un autre pour une requ\u00eate donn\u00e9e, pour un besoin d'information donn\u00e9e."
        },
        {
            "start": 358,
            "end": 369,
            "text": "Ces jugements de pr\u00e9f\u00e9rence vont toujours \u00eatre conditionn\u00e9s par un contexte qui est la requ\u00eate qu'on a soumise au moteur de recherche."
        },
        {
            "start": 369,
            "end": 382,
            "text": "Le principe, c'est de partir sur cette id\u00e9e que les jugements par paire sont des jugements qui sont plus g\u00e9n\u00e9raux, et donc d'exprimer ce jugement comme une pr\u00e9f\u00e9rence d'un document par rapport \u00e0 un autre."
        },
        {
            "start": 382,
            "end": 393,
            "text": "\u00c9tant donn\u00e9 une requ\u00eate Q, \u00e0 chaque paire de documents DI, DJ, on va associer une valeur de pr\u00e9f\u00e9rence YIJ qui va \u00eatre \u00e0 valeur dans le moins 1."
        },
        {
            "start": 393,
            "end": 419,
            "text": "Et typiquement, on va dire que YIJ va \u00eatre \u00e9gal \u00e0 1 si XI est pr\u00e9f\u00e9r\u00e9 \u00e0 XJ pour la requ\u00eate Q. Cette relation de pr\u00e9f\u00e9rence, on va la noter de cette mani\u00e8re-l\u00e0, et donc typiquement, il faut lire cette \u00e9quation comme XI est pr\u00e9f\u00e9r\u00e9 \u00e0 XJ pour la requ\u00eate Q, toujours \u00e9tant donn\u00e9 un contexte donn\u00e9 qui est ici notre requ\u00eate Q. Voil\u00e0."
        },
        {
            "start": 419,
            "end": 428,
            "text": "Et donc, dans l'approche par paire, on va chercher \u00e0 d\u00e9terminer non plus le score de pertinence, mais quel document est plus pertinent qu'un autre."
        },
        {
            "start": 428,
            "end": 433,
            "text": "On va essayer de d\u00e9terminer la pr\u00e9f\u00e9rence d'un document par rapport \u00e0 un autre."
        },
        {
            "start": 433,
            "end": 439,
            "text": "Regardons de mani\u00e8re un peu plus d\u00e9taill\u00e9e comment on va pouvoir mettre ce principe en \u0153uvre."
        },
        {
            "start": 439,
            "end": 457,
            "text": "L'\u00e9l\u00e9ment fondamental, \u00e7a reste toujours ce couple, ou cette paire document-requ\u00eate qu'on appelle XI, et donc on consid\u00e8re qu'on a N couples, N \u00e9chantillons de paire document-requ\u00eate."
        },
        {
            "start": 457,
            "end": 460,
            "text": "\u00c7a, c'est notre ensemble d'apprentissage."
        },
        {
            "start": 460,
            "end": 481,
            "text": "Et ce qu'on souhaite trouver, c'est une fonction d'ordonnancement, c'est-\u00e0-dire qu'on souhaite trouver une fonction f qui va respecter l'ordre des X. Donc si XJ est pr\u00e9f\u00e9r\u00e9 \u00e0 XI, alors f de XJ doit \u00eatre sup\u00e9rieur \u00e0 f de XI."
        },
        {
            "start": 481,
            "end": 490,
            "text": "Donc, pour simplifier le d\u00e9veloppement, je vais consid\u00e9rer ici f qui est une fonction de classement lin\u00e9aire, et donc que je vais pouvoir \u00e9crire comme \u00e7a."
        },
        {
            "start": 490,
            "end": 492,
            "text": "\u00c7a, c'est le produit Scalar."
        },
        {
            "start": 492,
            "end": 502,
            "text": "En pratique, f, \u00e7a peut \u00eatre tout type de fonction, mais vraiment pour plus facilement vous expliquer ce d\u00e9veloppement-l\u00e0, on va consid\u00e9rer que f, c'est une fonction de classement lin\u00e9aire."
        },
        {
            "start": 502,
            "end": 512,
            "text": "Pour f lin\u00e9aire, j'ai donc, si f de XI est sup\u00e9rieur \u00e0 f de XJ, alors j'ai bien cette relation d'\u00e9quivalence."
        },
        {
            "start": 512,
            "end": 521,
            "text": "Donc ma fonction de classement lin\u00e9aire, elle est positive dans ce cas-l\u00e0 pour la diff\u00e9rence XI moins XJ."
        },
        {
            "start": 521,
            "end": 526,
            "text": "Donc pour f lin\u00e9aire, j'ai bien cette relation d'\u00e9quivalence."
        },
        {
            "start": 526,
            "end": 534,
            "text": "Je peux donc transformer mon probl\u00e8me de classement en un probl\u00e8me de classification binaire sur la diff\u00e9rence des XI moins XJ."
        },
        {
            "start": 534,
            "end": 540,
            "text": "Et c'est comme \u00e7a, en fait, que va fonctionner l'approche par paire."
        },
        {
            "start": 540,
            "end": 543,
            "text": "Et \u00e7a, c'est ce qu'on appelle la transformation par paire."
        },
        {
            "start": 543,
            "end": 557,
            "text": "C'est-\u00e0-dire qu'on va former, \u00e0 partir de nos donn\u00e9es, des exemples d'apprentissage sous la forme de paires \u00e9tiquet\u00e9es en utilisant la diff\u00e9rence entre deux couples, donc XI moins XJ."
        },
        {
            "start": 557,
            "end": 581,
            "text": "Et les \u00e9tiquettes que je vais donner, typiquement, elles vont \u00eatre binaires et je vais donner \u00e0 une diff\u00e9rence XI moins XJ la valeur de 1 si XI est pr\u00e9f\u00e9r\u00e9 \u00e0 XJ pour la requ\u00eate Q."
        },
        {
            "start": 581,
            "end": 584,
            "text": "Et puis sinon, je lui donne la valeur moins 1."
        },
        {
            "start": 584,
            "end": 595,
            "text": "Voil\u00e0, donc j'ai bien ces relations d'\u00e9quivalence, c'est-\u00e0-dire que si XI est pr\u00e9f\u00e9r\u00e9 \u00e0 XJ, alors YIJ est \u00e9gal \u00e0 plus 1."
        },
        {
            "start": 595,
            "end": 601,
            "text": "Et donc, j'ai bien ramen\u00e9 comme \u00e7a mon probl\u00e8me de classement en un probl\u00e8me de classification binaire."
        },
        {
            "start": 601,
            "end": 609,
            "text": "Donc, on va rapidement illustrer le principe de cette transformation par paire sur un petit exemple."
        },
        {
            "start": 609,
            "end": 614,
            "text": "Donc, on consid\u00e8re ici qu'on a deux requ\u00eates."
        },
        {
            "start": 614,
            "end": 621,
            "text": "Donc \u00e7a, c'est les donn\u00e9es qui correspondent \u00e0 la requ\u00eate 1 et \u00e7a, c'est les donn\u00e9es qui correspondent \u00e0 la requ\u00eate 2."
        },
        {
            "start": 621,
            "end": 630,
            "text": "On a donc des donn\u00e9es qui sont class\u00e9es et qui sont class\u00e9es selon une fonction de classement qu'on voit \u00eatre lin\u00e9aire."
        },
        {
            "start": 630,
            "end": 645,
            "text": "Voil\u00e0, typiquement, si on projette les donn\u00e9es sur cette droite lin\u00e9aire, sur cet hyperplan lin\u00e9aire, on voit qu'on peut effectivement l'utiliser comme une fonction de classement."
        },
        {
            "start": 645,
            "end": 662,
            "text": "A partir de ces donn\u00e9es, je vais donc appliquer ma transform\u00e9e par paire en prenant donc l'ensemble des donn\u00e9es associ\u00e9es \u00e0 un contexte donn\u00e9, donc \u00e0 une requ\u00eate donn\u00e9e, et qui ont des rangs diff\u00e9rents et en effectuant les diff\u00e9rences par paire."
        },
        {
            "start": 662,
            "end": 678,
            "text": "Et donc l\u00e0, typiquement, si je prends la paire X1, X2, alors je vais labelliser X1, X2 en plus 1 parce que X1 est pr\u00e9f\u00e9r\u00e9 \u00e0 X2 pour cette requ\u00eate avec cette fonction de classement."
        },
        {
            "start": 678,
            "end": 690,
            "text": "Et de m\u00eame, X2 va \u00eatre pr\u00e9f\u00e9r\u00e9 \u00e0 X3 pour cette requ\u00eate avec cette fonction de classement et donc je vais \u00e9tiqueter X2 moins X3 \u00e0 plus 1."
        },
        {
            "start": 690,
            "end": 703,
            "text": "Si maintenant je prends la paire X3, X1, je lui donnerai le label moins 1 parce que X3 n'est pas pr\u00e9f\u00e9r\u00e9 \u00e0 X1 pour cette requ\u00eate avec cette fonction de classement."
        },
        {
            "start": 703,
            "end": 722,
            "text": "Et donc en appliquant cette fonction de transformation par paire \u00e0 l'ensemble des donn\u00e9es qui appartiennent \u00e0 des rangs diff\u00e9rents et \u00e0 un m\u00eame contexte, alors je transforme effectivement mon probl\u00e8me en un probl\u00e8me de classification binaire."
        },
        {
            "start": 722,
            "end": 731,
            "text": "Donc on voit ici, pour les m\u00eames donn\u00e9es, le r\u00e9sultat de l'application de cette transformation."
        },
        {
            "start": 731,
            "end": 751,
            "text": "Donc on a bien ici nos instances positives et on a bien ici nos instances n\u00e9gatives et on voit sur cet exemple qu'on doit tr\u00e8s certainement pouvoir trouver une solution \u00e0 ce probl\u00e8me de classification en utilisant une technique de classification et notamment par exemple en utilisant des s\u00e9parateurs lin\u00e9aires."
        },
        {
            "start": 751,
            "end": 769,
            "text": "Voil\u00e0, donc juste des petites remarques, dans la constitution du jeu de donn\u00e9es, on ne consid\u00e8re que les donn\u00e9es d'un m\u00eame groupe qui sont prises en compte pour les paires de diff\u00e9rence et bien s\u00fbr, seules les donn\u00e9es de rang diff\u00e9rent dans ce m\u00eame groupe sont prises en compte."
        },
        {
            "start": 769,
            "end": 779,
            "text": "Je ne fais pas, je ne construis pas de paire de diff\u00e9rence en prenant des donn\u00e9es qui sont de groupe diff\u00e9rent, \u00e7a n'aurait pas de sens, c'est des donn\u00e9es qui sont non comparables."
        },
        {
            "start": 779,
            "end": 798,
            "text": "Voil\u00e0, donc je vais juste terminer pour cette pr\u00e9sentation de l'approche par paire en pr\u00e9sentant une approche qui s'appelle Ranking SVM et qui se base sur les s\u00e9parateurs \u00e0 vaste marge pour proposer une solution \u00e0 ce probl\u00e8me d'ordonnancement \u00e0 l'aide de cette transform\u00e9e par paire."
        },
        {
            "start": 798,
            "end": 806,
            "text": "Rapidement avant cela, je vais vous pr\u00e9senter ce que sont les s\u00e9parateurs \u00e0 vaste marge lin\u00e9aires."
        },
        {
            "start": 806,
            "end": 818,
            "text": "J'avais rapidement commenc\u00e9 \u00e0 pr\u00e9senter les SVM dans le cours pr\u00e9c\u00e9dent, notamment en pr\u00e9sentant ces m\u00e9thodes comme des m\u00e9thodes de classification dites discriminantes."
        },
        {
            "start": 818,
            "end": 832,
            "text": "Donc on a des donn\u00e9es d'apprentissage labellis\u00e9es, donc X, Y et on va se placer dans un cadre de classification binaire, donc avec des labels qui peuvent prendre les valeurs moins 1 et plus 1."
        },
        {
            "start": 832,
            "end": 866,
            "text": "Et je cherche \u00e0 construire \u00e0 partir de cet ensemble d'apprentissage, donc une fonction de d\u00e9cision qui va donc de l'espace de repr\u00e9sentation de mes donn\u00e9es \u00e0 moins 1 plus 1 et qui va permettre de pr\u00e9dire la classe moins 1 ou plus 1 d'un point X qui appartient \u00e0 X, n'importe quel point X qui appartient \u00e0 X. Voil\u00e0, ma fonction de d\u00e9cision, elle va de Rd dans R, Rd \u00e9tant l'espace de description de mes donn\u00e9es d'entr\u00e9e."
        },
        {
            "start": 866,
            "end": 874,
            "text": "Et puis je peux utiliser le signe de cette fonction pour classer mes donn\u00e9es."
        },
        {
            "start": 874,
            "end": 891,
            "text": "Donc typiquement, je vais affecter X \u00e0 la classe moins 1 si f2x est inf\u00e9rieur \u00e0 0 et \u00e0 la classe plus 1 si le signe de ma fonction de d\u00e9cision est sup\u00e9rieur \u00e0 0, appliqu\u00e9 \u00e0 X bien s\u00fbr."
        },
        {
            "start": 891,
            "end": 910,
            "text": "Si on fait un bref rappel \u00e0 la mani\u00e8re dont je vous ai pr\u00e9sent\u00e9 la th\u00e9orie de l'apprentissage supervis\u00e9, j'avais \u00e9voqu\u00e9 le fait qu'on avait une classe d'hypoth\u00e8ses pour notre fonction f. Ici, cette classe d'hypoth\u00e8ses, c'est qu'on a des fonctions de d\u00e9cision qui sont lin\u00e9aires et donc qui vont prendre cette forme analytique."
        },
        {
            "start": 910,
            "end": 929,
            "text": "Et ce que je vais chercher \u00e0 d\u00e9terminer, ce sont les param\u00e8tres de cette fonction, c'est-\u00e0-dire le vecteur W et B. Voil\u00e0, prenons un exemple simple, pla\u00e7ons-nous dans R2 et prenons donc un probl\u00e8me de classification lin\u00e9aire."
        },
        {
            "start": 929,
            "end": 951,
            "text": "Donc on a effectivement deux classes qui sont caract\u00e9ris\u00e9es ici par les croix rouges et les ronds bleus et je cherche effectivement une fonction de s\u00e9paration dans cet espace R2 qui soit lin\u00e9aire."
        },
        {
            "start": 951,
            "end": 994,
            "text": "On voit ici que les donn\u00e9es, que le plan, je peux le s\u00e9parer en deux par un hyperplan dont l'\u00e9quation est WTX plus B. Si je prends le signe de cet hyperplan, \u00e7a me permet d'avoir une fonction qui va me permettre de classer mes diff\u00e9rents points selon le signe que prend cette fonction quand je l'applique \u00e0 mes donn\u00e9es X. Ce qu'il est int\u00e9ressant de voir, c'est que la distance d'un point \u00e0 cet hyperplan, on peut l'exprimer avec cette \u00e9quation."
        },
        {
            "start": 994,
            "end": 999,
            "text": "Notamment, on verra l'importance de cette notion-l\u00e0 un peu plus tard."
        },
        {
            "start": 999,
            "end": 1022,
            "text": "Et puis la distance de l'hyperplan \u00e0 l'origine, c'est la valeur absolue de B sur la norme de W. Si je reprends mon probl\u00e8me de s\u00e9paration binaire dans un espace \u00e0 deux dimensions, on voit qu'en fait j'ai plusieurs hyperplans qui sont possibles, plusieurs s\u00e9parateurs qui peuvent \u00eatre possibles."
        },
        {
            "start": 1022,
            "end": 1029,
            "text": "C'est-\u00e0-dire que cette droite-l\u00e0 va effectivement me permettre de s\u00e9parer correctement mes donn\u00e9es, celle-ci aussi et celle-ci aussi."
        },
        {
            "start": 1029,
            "end": 1035,
            "text": "Donc la question qu'il faut se poser ici, c'est quel est le s\u00e9parateur qui va \u00eatre optimal ?"
        },
        {
            "start": 1035,
            "end": 1046,
            "text": "Le s\u00e9parateur optimal, l'hyperplan optimal, \u00e7a va \u00eatre celui qui va classifier correctement les donn\u00e9es et qui se trouve le plus loin possible de tous les exemples."
        },
        {
            "start": 1046,
            "end": 1048,
            "text": "Et \u00e7a, c'est \u00e0 des fins de g\u00e9n\u00e9ralisation."
        },
        {
            "start": 1048,
            "end": 1054,
            "text": "Cet hyperplan optimal, c'est l'hyperplan de marge maximale."
        },
        {
            "start": 1054,
            "end": 1061,
            "text": "La marge \u00e9tant la distance minimale entre un exemple et l'hyperplan, et donc la surface de s\u00e9paration."
        },
        {
            "start": 1061,
            "end": 1065,
            "text": "C'est donc cet hyperplan optimal que je vais chercher \u00e0 d\u00e9terminer."
        },
        {
            "start": 1065,
            "end": 1070,
            "text": "Voil\u00e0, juste quelques d\u00e9finitions."
        },
        {
            "start": 1070,
            "end": 1077,
            "text": "On parle de s\u00e9parateur lin\u00e9aire, donc il est n\u00e9cessaire de d\u00e9finir ce que sont les donn\u00e9es qui sont s\u00e9parables lin\u00e9airement."
        },
        {
            "start": 1077,
            "end": 1089,
            "text": "Et typiquement, on va dire que les couples XYI vont \u00eatre lin\u00e9airement s\u00e9parables s'il existe un hyperplan qui permet de discriminer correctement cet ensemble de donn\u00e9es."
        },
        {
            "start": 1089,
            "end": 1093,
            "text": "Dans le cas contraire, on va parler d'exemples qui ne sont pas s\u00e9parables."
        },
        {
            "start": 1093,
            "end": 1109,
            "text": "Pour limiter l'espace des possibilit\u00e9s, on va consid\u00e9rer que les points les plus proches de l'hyperplan s\u00e9parateur sont situ\u00e9s sur des hyperplans qu'on va appeler canoniques et qui sont d\u00e9finis par cette \u00e9quation."
        },
        {
            "start": 1109,
            "end": 1117,
            "text": "L'\u00e9quation WTX plus B est \u00e9gale \u00e0 1 et WTX plus B est \u00e9gale \u00e0 moins 1."
        },
        {
            "start": 1117,
            "end": 1140,
            "text": "Dans ce cas, la marge, on peut la d\u00e9finir comme \u00e9tant 2 sur la norme de W. Et d'autre part, les points vont \u00eatre bien class\u00e9s si, quels que soient I, donc pour tous les points de mon ensemble d'apprentissage, j'ai Y fois F de XI qui est sup\u00e9rieur \u00e0 1."
        },
        {
            "start": 1140,
            "end": 1159,
            "text": "\u00c7a, c'est des contraintes, des conditions qui m'expriment que chaque point est bien class\u00e9 par ma fonction F. Et en utilisant cette simplification, je peux cette fois-ci d\u00e9finir de mani\u00e8re beaucoup plus formelle ce que c'est que mon s\u00e9parateur \u00e0 vaste marge."
        },
        {
            "start": 1159,
            "end": 1183,
            "text": "Donc, \u00e9tant donn\u00e9 un ensemble de points lin\u00e9airement s\u00e9parables et tiquet\u00e9s, donc mon ensemble d'apprentissage qui contient N \u00e9chantillons, l'objectif va \u00eatre de trouver un hyperplan qui maximise la marge et qui discrimine correctement les points de D. Donc, la formulation math\u00e9matique du SVM, c'est celle-ci."
        },
        {
            "start": 1183,
            "end": 1191,
            "text": "Je cherche \u00e0 minimiser selon WB 1 demi de la norme de W\u00b2."
        },
        {
            "start": 1191,
            "end": 1203,
            "text": "Donc \u00e7a, c'est typiquement ce qui exprime le fait que je cherche \u00e0 maximiser ma marge sous contrainte que l'ensemble de mes points de mon ensemble d'apprentissage soit bien class\u00e9."
        },
        {
            "start": 1203,
            "end": 1206,
            "text": "C'est ce qu'on exprime avec cette \u00e9quation-l\u00e0."
        },
        {
            "start": 1206,
            "end": 1212,
            "text": "\u00c7a, c'est la formulation classique des s\u00e9parateurs \u00e0 vaste marge."
        },
        {
            "start": 1212,
            "end": 1224,
            "text": "Voil\u00e0, on a donc ici un probl\u00e8me de minimisation sous contrainte qu'on peut r\u00e9soudre par des approches num\u00e9riques comme de la programmation quadratique, par exemple en minimisant le carr\u00e9 de la norme."
        },
        {
            "start": 1224,
            "end": 1231,
            "text": "Je ne vais pas rentrer dans les d\u00e9tails de r\u00e9solution de ce type de probl\u00e8me d'optimisation."
        },
        {
            "start": 1231,
            "end": 1243,
            "text": "Ce que je peux vous dire, c'est qu'en fait pour les SVM, on va faire un passage au lacrang\u00e9, c'est-\u00e0-dire qu'on va utiliser la forme duale de ce probl\u00e8me d'optimisation pr\u00e9c\u00e9dent."
        },
        {
            "start": 1243,
            "end": 1254,
            "text": "On sait qu'un probl\u00e8me d'optimisation, il poss\u00e8de une forme duale si la fonction objectif et les contraintes sont strictement convexes et alors dans ce cas l\u00e0 la solution du probl\u00e8me dual est la solution du probl\u00e8me original."
        },
        {
            "start": 1254,
            "end": 1259,
            "text": "Donc on utilise cette propri\u00e9t\u00e9 l\u00e0 pour la r\u00e9solution des s\u00e9parateurs \u00e0 vaste marge."
        },
        {
            "start": 1259,
            "end": 1276,
            "text": "Et notamment on va donc passer au lagrangien, en introduisant dans le probl\u00e8me d'optimisation des multiplicateurs de Lagrange \u03b1i qui sont donc associ\u00e9s aux diff\u00e9rentes contraintes d'in\u00e9galit\u00e9 qui expriment que l'ensemble des points de mon ensemble d'apprentissage doivent \u00eatre bien class\u00e9s."
        },
        {
            "start": 1276,
            "end": 1283,
            "text": "Donc du coup j'introduis dans mon probl\u00e8me, dans la formulation de mon probl\u00e8me, n param\u00e8tres \u03b1i."
        },
        {
            "start": 1283,
            "end": 1288,
            "text": "Et donc la formulation de mon probl\u00e8me en passant au lagrangien \u00e7a devient celle-ci."
        },
        {
            "start": 1288,
            "end": 1310,
            "text": "Donc j'ai cette fonction objectif qui devient celle-ci avec donc les param\u00e8tres, le param\u00e8tre W, B, \u03b1 et donc cette fonction objectif ici qui est donc je cherche \u00e0 maximiser ma marge avec donc les contraintes que l'ensemble des donn\u00e9es de mon ensemble d'apprentissage doivent \u00eatre bien class\u00e9es."
        },
        {
            "start": 1310,
            "end": 1312,
            "text": "J'exprime exactement la m\u00eame chose."
        },
        {
            "start": 1312,
            "end": 1321,
            "text": "Donc j'ai une nouvelle formulation du probl\u00e8me en fait o\u00f9 la contrainte elle est directement int\u00e9gr\u00e9e dans la fonction objectif et donc \u00e7a devient int\u00e9ressant."
        },
        {
            "start": 1321,
            "end": 1331,
            "text": "Voil\u00e0, pour la r\u00e9solution des SVM il est important d'introduire des vecteurs particuliers qui sont ce qu'on appelle des vecteurs supports."
        },
        {
            "start": 1331,
            "end": 1339,
            "text": "Donc W1 je peux le d\u00e9finir comme \u00e9tant la somme sur l'ensemble de mes donn\u00e9es d'apprentissage de \u03b1i, yi, xi."
        },
        {
            "start": 1339,
            "end": 1348,
            "text": "Je sais que \u03b1i est nulle si yi fois Wtxi plus B est sup\u00e9rieur \u00e0 1."
        },
        {
            "start": 1348,
            "end": 1356,
            "text": "Donc W il n'est d\u00e9fini que par les points tels que j'ai cette \u00e9quation l\u00e0 qui est \u00e9gale \u00e0 1."
        },
        {
            "start": 1356,
            "end": 1360,
            "text": "Et ces points l\u00e0 c'est mes vecteurs supports, ce sont ces points l\u00e0."
        },
        {
            "start": 1360,
            "end": 1369,
            "text": "Ces points qui finalement appartiennent au plan canonique qu'on a introduit plus en avant."
        },
        {
            "start": 1369,
            "end": 1384,
            "text": "Voil\u00e0, donc en pratique les SVM on calcule W, on cherche finalement les valeurs, on estime W en utilisant les donn\u00e9es d'apprentissage pour r\u00e9soudre le dual, donc le Lagrangien."
        },
        {
            "start": 1384,
            "end": 1387,
            "text": "Et en faisant \u00e7a on obtient les param\u00e8tres \u03b1i."
        },
        {
            "start": 1387,
            "end": 1393,
            "text": "A partir des \u03b1i j'en d\u00e9duis donc l'estimation de mon vecteur W avec cette \u00e9quation l\u00e0."
        },
        {
            "start": 1393,
            "end": 1398,
            "text": "La somme pour l'ensemble de mes donn\u00e9es d'apprentissage de \u03b1i, xi, yi, xi."
        },
        {
            "start": 1398,
            "end": 1408,
            "text": "Puis je peux du coup calculer B en prenant en compte le fait que les \u03b1i qui sont sup\u00e9rieurs \u00e0 0 correspondent aux points supports qui v\u00e9rifient donc cette relation."
        },
        {
            "start": 1408,
            "end": 1411,
            "text": "Et donc je peux en d\u00e9duire comme \u00e7a la valeur de B."
        },
        {
            "start": 1411,
            "end": 1420,
            "text": "En pratique je fais la moyenne de ces termes pour l'ensemble des vecteurs supports SV pour obtenir une valeur num\u00e9rique qui est stable."
        },
        {
            "start": 1420,
            "end": 1451,
            "text": "Et donc j'obtiens ma fonction de d\u00e9cision que je peux exprimer comme \u00e7a, qui est donc la somme pour l'ensemble des vecteurs supports de \u03b1i, xi, xi, t, x. J'ai ici un produit scalaire en fait, je pourrais aussi l'exprimer avec l'autre notation des produits scalaires, plus B. Donc tout ce qu'on a vu c'est assez joli, mais \u00e7a ne marche que si on a des donn\u00e9es qui sont s\u00e9parables lin\u00e9airement."
        },
        {
            "start": 1451,
            "end": 1456,
            "text": "Or dans la vraie vie les donn\u00e9es elles sont rarement s\u00e9parables lin\u00e9airement."
        },
        {
            "start": 1456,
            "end": 1463,
            "text": "Il n'y a pas de raison qui fait que les donn\u00e9es sont s\u00e9parables lin\u00e9airement pour \u00e9norm\u00e9ment de probl\u00e8mes."
        },
        {
            "start": 1463,
            "end": 1472,
            "text": "Et donc on va voir comment on peut \u00e9tendre les s\u00e9parateurs \u00e0 basse de marge au cas non s\u00e9parable."
        },
        {
            "start": 1472,
            "end": 1483,
            "text": "Donc typiquement comment on va pouvoir modifier la formulation quand justement l'hypoth\u00e8se de s\u00e9parabilit\u00e9 lin\u00e9aire des points n'est pas v\u00e9rifi\u00e9e ?"
        },
        {
            "start": 1483,
            "end": 1497,
            "text": "Voil\u00e0, typiquement dans ce genre de cas l\u00e0, les points blancs qui sont ici et les carr\u00e9s verts qui sont ici font qu'on n'a pas des donn\u00e9es qui sont s\u00e9parables lin\u00e9airement."
        },
        {
            "start": 1497,
            "end": 1506,
            "text": "Dans ce cas l\u00e0, on va rel\u00e2cher les contraintes qui expriment le fait que les points sont bien class\u00e9s."
        },
        {
            "start": 1506,
            "end": 1515,
            "text": "Donc ces contraintes l\u00e0, et on va rajouter dans ces contraintes des variables de rel\u00e2chement \u03b5i."
        },
        {
            "start": 1515,
            "end": 1520,
            "text": "Et puis bien s\u00fbr on va p\u00e9naliser ces rel\u00e2chements dans la fonction objectif."
        },
        {
            "start": 1520,
            "end": 1526,
            "text": "Et donc on obtient une nouvelle formulation des SVM dans le cas non s\u00e9parable qui est celle-ci."
        },
        {
            "start": 1526,
            "end": 1542,
            "text": "Donc on cherche toujours \u00e0 maximiser notre marge, \u00e0 trouver les valeurs des param\u00e8tres W et B qui maximisent notre marge, sous contrainte rel\u00e2ch\u00e9e que les points sont bien class\u00e9s."
        },
        {
            "start": 1542,
            "end": 1550,
            "text": "Donc l\u00e0 on introduit nos variables de rel\u00e2chement qu'on p\u00e9nalise dans la fonction objectif, ici."
        },
        {
            "start": 1550,
            "end": 1565,
            "text": "Et donc ce C l\u00e0, c'est une contrainte de r\u00e9gularisation justement li\u00e9e \u00e0 cette p\u00e9nalisation des variables de rel\u00e2chement, qui est un hyperparam\u00e8tre des SVM."
        },
        {
            "start": 1565,
            "end": 1574,
            "text": "Et puis on a encore un cas qui est un peu plus compliqu\u00e9, qui est le cas o\u00f9 la s\u00e9paration n'est pas lin\u00e9aire."
        },
        {
            "start": 1574,
            "end": 1589,
            "text": "Et donc on voit bien qu'ici la droite qui s\u00e9pare, la fonction qui s\u00e9pare nos points, qui sont s\u00e9parables, mais cette fonction l\u00e0 n'est clairement pas lin\u00e9aire."
        },
        {
            "start": 1589,
            "end": 1592,
            "text": "Comment on peut traiter ce cas ?"
        },
        {
            "start": 1592,
            "end": 1597,
            "text": "Donc on va prendre un exemple aussi jou\u00e9, un exemple synth\u00e9tique qui est celui-l\u00e0."
        },
        {
            "start": 1597,
            "end": 1604,
            "text": "On voit qu'effectivement on a des donn\u00e9es qui sont dans un espace en deux dimensions, qui ne sont pas s\u00e9parables lin\u00e9airement."
        },
        {
            "start": 1604,
            "end": 1613,
            "text": "Et pour traiter ce probl\u00e8me-l\u00e0, avec les SVM, on va utiliser une astuce qu'on appelle l'astuce des noyaux."
        },
        {
            "start": 1613,
            "end": 1622,
            "text": "Typiquement, on va \u00e9tendre notre probl\u00e8me \u00e0 un cas o\u00f9 les donn\u00e9es vont \u00eatre s\u00e9parables lin\u00e9airement."
        },
        {
            "start": 1622,
            "end": 1626,
            "text": "\u00c7a c'est une extension justement des SVM \u00e0 des s\u00e9parateurs non lin\u00e9aires."
        },
        {
            "start": 1626,
            "end": 1635,
            "text": "Et pour \u00e7a, on va transposer les donn\u00e9es dans un autre espace, dans lequel les donn\u00e9es vont \u00eatre cette fois-ci lin\u00e9airement s\u00e9parables."
        },
        {
            "start": 1635,
            "end": 1642,
            "text": "Donc on va appliquer \u00e0 nos donn\u00e9es une transformation qui va de Rd dans H, un espace de Hilbert."
        },
        {
            "start": 1642,
            "end": 1655,
            "text": "Cette transformation, c'est le phi ici, qui est telle qu'on part d'un espace de d\u00e9part dans lequel les donn\u00e9es ne sont pas s\u00e9parables lin\u00e9airement, vers un espace d'arriv\u00e9e o\u00f9 elles le sont."
        },
        {
            "start": 1655,
            "end": 1666,
            "text": "Voil\u00e0, si on revient \u00e0 notre exemple, on a bien un espace initial \u00e0 deux dimensions dans lequel il n'est pas possible de s\u00e9parer lin\u00e9airement nos donn\u00e9es."
        },
        {
            "start": 1666,
            "end": 1686,
            "text": "Et donc je dois choisir une transformation phi qui doit permettre une s\u00e9paration lin\u00e9aire dans un nouvel espace H. Ici, je peux par exemple prendre cette transformation phi, qui d\u00e9pend donc de X1 et X2, et qui transforme mes donn\u00e9es dans un espace \u00e0 trois dimensions, avec X1, X2 et X1\u00b2 plus X2\u00b2."
        },
        {
            "start": 1686,
            "end": 1703,
            "text": "Si je trace mes donn\u00e9es dans ce nouvel espace, qui est un espace \u00e0 trois dimensions, qui est un espace de plus grande dimension que l'espace original, on voit qu'ici j'ai effectivement une possibilit\u00e9 de s\u00e9parer mes donn\u00e9es par un s\u00e9parateur lin\u00e9aire."
        },
        {
            "start": 1703,
            "end": 1705,
            "text": "On voit qu'on a effectivement un hyper-s\u00e9parateur."
        },
        {
            "start": 1705,
            "end": 1715,
            "text": "C'est vraiment cette id\u00e9e-l\u00e0 qu'on veut mettre en place pour utiliser les s\u00e9parateurs \u00e0 vaste marge quand on a des cas de s\u00e9paration non lin\u00e9aire."
        },
        {
            "start": 1715,
            "end": 1724,
            "text": "Ce qui est compliqu\u00e9, c'est finalement le choix de cette transformation, le changement de repr\u00e9sentation, comment je peux faire."
        },
        {
            "start": 1724,
            "end": 1732,
            "text": "On souhaite avoir un changement de repr\u00e9sentation qui va permettre une s\u00e9paration lin\u00e9aire entre nos deux classes, tout en respectant bien s\u00fbr la vraie similarit\u00e9 entre les donn\u00e9es."
        },
        {
            "start": 1732,
            "end": 1740,
            "text": "En g\u00e9n\u00e9ral, \u00e7a veut dire que je dois trouver ce qu'on appelle un espace de redescription de plus grande dimension."
        },
        {
            "start": 1740,
            "end": 1743,
            "text": "Les questions, c'est comment je peux faire \u00e7a ?"
        },
        {
            "start": 1743,
            "end": 1752,
            "text": "Je peux faire \u00e7a par exemple par un s\u00e9parateur lin\u00e9aire, je dois trouver ce qu'on appelle un espace de redescription de plus grande dimension."
        },
        {
            "start": 1752,
            "end": 1760,
            "text": "Les questions, c'est comment je peux trouver cet espace de redescription et comment je peux garantir la r\u00e9alisation des calculs ?"
        },
        {
            "start": 1760,
            "end": 1775,
            "text": "Pour \u00e7a, c'est l\u00e0 qu'il y a vraiment l'astuce des fonctions noyaux, qui fait qu'on va \u00e9viter de calculer explicitement la transformation \u03a6, et on va plut\u00f4t s'appuyer sur des fonctions particuli\u00e8res qui sont des fonctions noyaux."
        },
        {
            "start": 1775,
            "end": 1793,
            "text": "Plut\u00f4t qu'appliquer la transformation de changement de repr\u00e9sentation, on va d\u00e9finir une fonction noyau K, telle que K de XI, XJ, c'est \u00e9gal au produit scalaire de la transform\u00e9e de XI et de la transform\u00e9e de XJ."
        },
        {
            "start": 1793,
            "end": 1797,
            "text": "Ce produit scalaire dans l'espace de redescription."
        },
        {
            "start": 1797,
            "end": 1804,
            "text": "Je cherche une fonction K qui correspond au produit scalaire dans mon espace H, qui est mon espace de redescription."
        },
        {
            "start": 1804,
            "end": 1810,
            "text": "Il y a des travaux th\u00e9oriques qui prouvent que de telles fonctions existent."
        },
        {
            "start": 1810,
            "end": 1820,
            "text": "C'est notamment le th\u00e9or\u00e8me de Mercer qui dit qu'une fonction noyau K continue sym\u00e9trique et semi-d\u00e9finie positive peut s'exprimer comme un produit scalaire dans un espace de plus grande dimension."
        },
        {
            "start": 1820,
            "end": 1827,
            "text": "En utilisant cette fonction noyau, la fonction de d\u00e9cision dans l'espace d'origine devient celle-ci."
        },
        {
            "start": 1827,
            "end": 1856,
            "text": "La somme pour Y qui appartient au vecteur support de \u03b1I, YI, la fonction noyau appliqu\u00e9e \u00e0 XI, X, plus P. On appelle la fonction noyau toute fonction K, qui va de mon espace originelle dans R, et qui peut \u00eatre interpr\u00e9t\u00e9e comme un produit scalaire dans un plongement \u03a6."
        },
        {
            "start": 1856,
            "end": 1878,
            "text": "Le gros avantage, c'est qu'on peut maintenant appliquer tous les algorithmes qu'on a vus pr\u00e9c\u00e9demment de s\u00e9paration optimale avec marge souple ou dure, donc avec les contraintes de rel\u00e2chement, en rempla\u00e7ant le produit scalaire entre XI et XJ par la fonction noyau K de XI, XJ."
        },
        {
            "start": 1878,
            "end": 1886,
            "text": "J'obtiens ainsi un classifieur exprim\u00e9 comme \u00e7a, et qui est lui lin\u00e9aire dans l'espace de plongement."
        },
        {
            "start": 1886,
            "end": 1896,
            "text": "Il existe \u00e9norm\u00e9ment de travaux sur des fonctions noyaux, il existe \u00e9norm\u00e9ment de fonctions noyaux, par exemple les noyaux polynomials, le noyau gaussien, le noyau sigmo\u00efde, etc."
        },
        {
            "start": 1896,
            "end": 1907,
            "text": "Le choix du noyau est tr\u00e8s important dans la mise en \u0153uvre des SVM, parce qu'il doit maximiser les chances d'\u00eatre dans le bon espace."
        },
        {
            "start": 1907,
            "end": 1914,
            "text": "\u00c7a fait aussi partie des hyper param\u00e8tres des SVM, le choix du noyau."
        },
        {
            "start": 1914,
            "end": 1922,
            "text": "Enfin, si je veux en finir avec les SVM, je dois parler du cas o\u00f9 on a plusieurs classes."
        },
        {
            "start": 1922,
            "end": 1928,
            "text": "On n'a parl\u00e9 ici que des SVM dans le cadre binaire, c'est un probl\u00e8me de classification binaire."
        },
        {
            "start": 1928,
            "end": 1937,
            "text": "Comment transformer tout ce qu'on a vu pr\u00e9c\u00e9demment, ou comment l'adapter \u00e0 un cas o\u00f9 j'ai plusieurs classes ?"
        },
        {
            "start": 1937,
            "end": 1944,
            "text": "Ici, je n'ai plus deux classes, mais j'ai ces classes CI, et donc typiquement, telle que c'est, et plus grande que 2."
        },
        {
            "start": 1944,
            "end": 1949,
            "text": "Il y a plusieurs strat\u00e9gies pour appliquer les SVM au cadre multiclasse."
        },
        {
            "start": 1949,
            "end": 1952,
            "text": "Il y a une strat\u00e9gie qui s'appelle le one versus all."
        },
        {
            "start": 1952,
            "end": 1977,
            "text": "\u00c7a consiste \u00e0 prendre pour une classe CI ses exemples positifs, et de ramener un probl\u00e8me de classification binaire en prenant tous les exemples des autres classes CI diff\u00e9rents de I qui valent moins 1."
        },
        {
            "start": 1977,
            "end": 1980,
            "text": "Je me ram\u00e8ne ainsi un probl\u00e8me de classification binaire."
        },
        {
            "start": 1980,
            "end": 1983,
            "text": "Je vais apprendre ces classifiers binaires."
        },
        {
            "start": 1983,
            "end": 1993,
            "text": "Pour faire mon classement, je vais retenir la classe qui a le plus fort score comme classe pour mes donn\u00e9es."
        },
        {
            "start": 1993,
            "end": 1997,
            "text": "Une autre strat\u00e9gie s'appelle le one versus one."
        },
        {
            "start": 1997,
            "end": 2006,
            "text": "Cette fois-ci, je vais prendre les exemples positifs d'une classe I1 et les exemples n\u00e9gatifs d'une classe I2."
        },
        {
            "start": 2006,
            "end": 2011,
            "text": "Je vais apprendre de cette mani\u00e8re ces fois-ci moins 1 sur 2 classifiers binaires."
        },
        {
            "start": 2011,
            "end": 2016,
            "text": "Il faut bien s\u00fbr que je fasse s'affronter mes diff\u00e9rents classifiers."
        },
        {
            "start": 2016,
            "end": 2022,
            "text": "Je vais mettre en place un processus de tournoi et je vais voter pour chaque classifier."
        },
        {
            "start": 2022,
            "end": 2024,
            "text": "Une classe va gagner \u00e0 chaque fois."
        },
        {
            "start": 2024,
            "end": 2034,
            "text": "En agr\u00e9geant mes votes, je vais obtenir le vainqueur et la classe que je vais pouvoir affecter \u00e0 mes donn\u00e9es."
        },
        {
            "start": 2034,
            "end": 2037,
            "text": "Un petit bilan sur les SVM."
        },
        {
            "start": 2037,
            "end": 2042,
            "text": "C'est une approche d'apprentissage qui est relativement puissante et qui est capable de trouver des motifs non lin\u00e9aires."
        },
        {
            "start": 2042,
            "end": 2045,
            "text": "C'est int\u00e9ressant avec deux id\u00e9es principales."
        },
        {
            "start": 2045,
            "end": 2051,
            "text": "Cette maximisation de la marge entre la fronti\u00e8re de d\u00e9cision et les exemples les plus proches, qui sont les vecteurs supports."
        },
        {
            "start": 2051,
            "end": 2063,
            "text": "Et puis l'astuce des noyaux avec une redescription des observations dans un nouvel espace o\u00f9 une s\u00e9paration lin\u00e9aire sera possible dans le cas o\u00f9 elle ne l'est pas dans l'espace originel."
        },
        {
            "start": 2063,
            "end": 2067,
            "text": "Et puis pour finir, une extension facile au cas multiclasse."
        },
        {
            "start": 2067,
            "end": 2077,
            "text": "Apr\u00e8s cette grosse parenth\u00e8se sur les SVM, je peux revenir \u00e0 mon probl\u00e8me d'ordonnancement avec l'approche de transformer par paire."
        },
        {
            "start": 2077,
            "end": 2080,
            "text": "Et donc vous pr\u00e9senter la m\u00e9thode ranking SVM."
        },
        {
            "start": 2080,
            "end": 2093,
            "text": "Typiquement, je peux directement utiliser cette m\u00e9thode des SVM pour trouver un hyperplan s\u00e9parateur de mes donn\u00e9es g\u00e9n\u00e9r\u00e9es en appliquant la transform\u00e9e par paire."
        },
        {
            "start": 2093,
            "end": 2102,
            "text": "Mes donn\u00e9es d'apprentissage, c'est ppaire x1, x2, labellis\u00e9es comme pr\u00e9sent\u00e9es pr\u00e9c\u00e9demment."
        },
        {
            "start": 2102,
            "end": 2110,
            "text": "La formulation de ce probl\u00e8me en utilisant les SVM devient celle-l\u00e0."
        },
        {
            "start": 2110,
            "end": 2114,
            "text": "Je cherche \u00e0 maximiser ma marge."
        },
        {
            "start": 2114,
            "end": 2121,
            "text": "Ici, c'est la formulation avec des marges souples, avec mes contraintes de rel\u00e2chement."
        },
        {
            "start": 2121,
            "end": 2141,
            "text": "Sous contrainte que mes ppaires d'apprentissage, exprim\u00e9es comme la diff\u00e9rence de mes paires originelles, soient bien class\u00e9es, avec mes contraintes de rel\u00e2chement, etc."
        },
        {
            "start": 2141,
            "end": 2149,
            "text": "Je sais r\u00e9soudre ce type de probl\u00e8me avec des techniques d'optimisation."
        },
        {
            "start": 2149,
            "end": 2162,
            "text": "La solution du probl\u00e8me me donne une solution optimale, qui est finalement l'estimation de W. Comment je peux classer ?"
        },
        {
            "start": 2162,
            "end": 2175,
            "text": "Je sais que x' est pr\u00e9f\u00e9r\u00e9 \u00e0 x si et seulement si le signe du classifiaire est n\u00e9gatif."
        },
        {
            "start": 2175,
            "end": 2182,
            "text": "Si le signe de W \u00e9toile x-x' est n\u00e9gatif."
        },
        {
            "start": 2182,
            "end": 2190,
            "text": "Je pourrais classer en utilisant cette \u00e9quation et ce m\u00e9canisme."
        },
        {
            "start": 2190,
            "end": 2196,
            "text": "On va directement utiliser le score du mod\u00e8le qu'on vient d'apprendre."
        },
        {
            "start": 2196,
            "end": 2201,
            "text": "Cela r\u00e9pond aussi \u00e0 la deuxi\u00e8me question, qui est comment trier les documents pour de nouvelles requ\u00eates."
        },
        {
            "start": 2201,
            "end": 2217,
            "text": "Pour chaque requ\u00eate, je vais consid\u00e9rer mon x' qui vaut le vecteur nul, et je vais trier le document sur le score obtenu directement par la fonction W \u00e9toile x-x'."
        },
        {
            "start": 2217,
            "end": 2225,
            "text": "Ce score obtenu est celui-l\u00e0, avec x qui est toujours une paire document requ\u00eate."
        },
        {
            "start": 2225,
            "end": 2231,
            "text": "Cela ne change pas, c'est toujours l'\u00e9l\u00e9ment principal de ces approches Learning to Rank."
        },
        {
            "start": 2231,
            "end": 2244,
            "text": "Il y a bien s\u00fbr plein d'autres approches pour faire des strat\u00e9gies Learning to Rank \u00e0 partir de l'approche par paire, que je ne pr\u00e9senterai pas ici, mais juste pour r\u00e9sumer l'approche par paire."
        },
        {
            "start": 2244,
            "end": 2253,
            "text": "On a des donn\u00e9es d'entr\u00e9e qui sont des donn\u00e9es ordonn\u00e9es, mais de paire de vecteurs."
        },
        {
            "start": 2253,
            "end": 2260,
            "text": "On consid\u00e8re les documents par paire et pas les documents ind\u00e9pendamment comme dans l'approche par point."
        },
        {
            "start": 2260,
            "end": 2268,
            "text": "Je me ram\u00e8ne \u00e0 un probl\u00e8me de classification binaire avec l'approche de transformer par paire."
        },
        {
            "start": 2268,
            "end": 2277,
            "text": "J'apprends ce classifieur qui, en utilisant son score, va me permettre de faire le classement."
        },
        {
            "start": 2277,
            "end": 2286,
            "text": "J'ai ici une fonction de co\u00fbt paireoise de classification."
        },
        {
            "start": 2286,
            "end": 2293,
            "text": "Pour le classement, j'ai juste \u00e0 trier les valeurs de sortie de mon classifieur."
        },
        {
            "start": 2293,
            "end": 2305,
            "text": "Je peux donc proposer \u00e0 l'utilisateur une liste ordonn\u00e9e de documents, que j'\u00e9value avec les mesures d'\u00e9valuation classique d'ordonnancement."
        },
        {
            "start": 2305,
            "end": 2314,
            "text": "La derni\u00e8re approche possible pour l'apprentissage par ordonnancement, c'est l'approche par liste."
        },
        {
            "start": 2314,
            "end": 2320,
            "text": "Le principe est simple, c'est de traiter directement les listes tri\u00e9es comme des exemples d'apprentissage."
        },
        {
            "start": 2320,
            "end": 2323,
            "text": "Directement d'apprendre \u00e0 partir de listes tri\u00e9es."
        },
        {
            "start": 2323,
            "end": 2326,
            "text": "Pour \u00e7a, il y a deux types d'approches."
        },
        {
            "start": 2326,
            "end": 2333,
            "text": "Il y a des approches qui ont dans leur fonction objectif des \u00e9l\u00e9ments li\u00e9s aux mesures d'\u00e9valuation."
        },
        {
            "start": 2333,
            "end": 2340,
            "text": "Par exemple, il y a une approche qui s'appelle SVM Map, qui utilise des vecteurs SVM."
        },
        {
            "start": 2340,
            "end": 2354,
            "text": "La fonction objectif est la borne sup\u00e9rieure de la mesure d'\u00e9valuation Map, le mean average precision, dont on a parl\u00e9 pendant le cours d'\u00e9valuation."
        },
        {
            "start": 2354,
            "end": 2359,
            "text": "Il y a aussi des fonctions objectives qui sont directement d\u00e9finies sur des listes de documents."
        },
        {
            "start": 2359,
            "end": 2365,
            "text": "C'est compliqu\u00e9 ces mesures-l\u00e0, parce qu'en g\u00e9n\u00e9ral, on est sur des mesures d'\u00e9valuation non continues."
        },
        {
            "start": 2365,
            "end": 2371,
            "text": "On rentre dans des probl\u00e9matiques de diff\u00e9renciabilit\u00e9 et \u00e7a devient un peu compliqu\u00e9."
        },
        {
            "start": 2371,
            "end": 2383,
            "text": "Je ne pr\u00e9senterai pas d'approche tr\u00e8s d\u00e9taill\u00e9e sur cette approche par liste, parce que chaque approche n\u00e9cessiterait de gros d\u00e9veloppements."
        },
        {
            "start": 2383,
            "end": 2390,
            "text": "Le petit bilan sur les approches par liste, avec un m\u00eame m\u00e9canisme pour l'apprentissage et le classement."
        },
        {
            "start": 2390,
            "end": 2405,
            "text": "On fait une permutation sur cette fonction F, et on a donc une fonction de co\u00fbt qui est la m\u00eame pour l'apprentissage et le classement."
        },
        {
            "start": 2405,
            "end": 2412,
            "text": "Parce que typiquement, on s'appuie sur les mesures d'\u00e9valuation de classement pour construire cette fonction de co\u00fbt."
        },
        {
            "start": 2412,
            "end": 2420,
            "text": "On a vu que quand on est dans des approches \u00e0 base d'apprentissage, il est n\u00e9cessaire d'avoir des donn\u00e9es d'apprentissage."
        },
        {
            "start": 2420,
            "end": 2426,
            "text": "La question c'est, quelles sont les donn\u00e9es d'apprentissage \u00e0 disposition pour mettre en \u0153uvre ce type d'approche ?"
        },
        {
            "start": 2426,
            "end": 2432,
            "text": "Il existe un certain nombre de donn\u00e9es de benchmark qui sont disponibles."
        },
        {
            "start": 2432,
            "end": 2444,
            "text": "Il y a des donn\u00e9es qui sont annot\u00e9es pour plusieurs collections, notamment les collections TREC, qui sont les challenges classiques en \u00e9valuation dans la recherche d'informations, cl\u00e9 pour tout ce qui est multilinguisme."
        },
        {
            "start": 2444,
            "end": 2447,
            "text": "NTCIR, c'est plut\u00f4t des donn\u00e9es m\u00e9dicales."
        },
        {
            "start": 2447,
            "end": 2453,
            "text": "Les TOR, qui sont vraiment des donn\u00e9es qui ont \u00e9t\u00e9 mises \u00e0 disposition pour les paradigmes Learning to Rank."
        },
        {
            "start": 2453,
            "end": 2458,
            "text": "Et puis, il y a Yahoo qui a aussi mis \u00e0 disposition beaucoup de donn\u00e9es."
        },
        {
            "start": 2458,
            "end": 2466,
            "text": "Pour les entreprises, c'est un peu plus compliqu\u00e9 parce qu'on n'a pas de telles donn\u00e9es en g\u00e9n\u00e9ral, et il faut pouvoir les constituer."
        },
        {
            "start": 2466,
            "end": 2476,
            "text": "Dans le cas du web, c'est un petit peu diff\u00e9rent parce qu'on a une information tr\u00e8s riche qui provient du clic des utilisateurs."
        },
        {
            "start": 2476,
            "end": 2485,
            "text": "Et \u00e0 partir de cette information de clic, on peut inf\u00e9rer des pr\u00e9f\u00e9rences entre documents, et donc on peut inf\u00e9rer tr\u00e8s facilement des paires de pr\u00e9f\u00e9rences."
        },
        {
            "start": 2485,
            "end": 2496,
            "text": "C'est pour \u00e7a que l'approche par paire est tr\u00e8s d\u00e9velopp\u00e9e dans les approches d'apprentissage pour l'ARI, et notamment dans le contexte du web."
        },
        {
            "start": 2496,
            "end": 2505,
            "text": "Alors, il faut faire attention bien s\u00fbr parce que les clics, ils ne fournissent pas de jugement de pertinence absolue mais relatif, et donc c'est ce qu'on veut pour l'approche par paire."
        },
        {
            "start": 2505,
            "end": 2515,
            "text": "Et donc typiquement, il y a \u00e9norm\u00e9ment de strat\u00e9gies qui peuvent \u00eatre mises en place pour constituer ces donn\u00e9es d'apprentissage."
        },
        {
            "start": 2515,
            "end": 2522,
            "text": "Donc consid\u00e9rons un ordre D1, Dn par exemple, et c'est un ensemble de documents cliqu\u00e9s."
        },
        {
            "start": 2522,
            "end": 2530,
            "text": "Alors, on pourrait par exemple utiliser cette strat\u00e9gie pour construire un ordre de pertinence entre documents."
        },
        {
            "start": 2530,
            "end": 2539,
            "text": "Donc si un document Di appartient \u00e0 C et que Di n'appartient pas \u00e0 C, forc\u00e9ment Di est pr\u00e9f\u00e9r\u00e9 pour la requ\u00eate Q \u00e0 Di."
        },
        {
            "start": 2539,
            "end": 2552,
            "text": "Voil\u00e0, si Di c'est le dernier document cliqu\u00e9, et quel que soit J qui est inf\u00e9rieur \u00e0 I, alors Di n'appartient pas \u00e0 C, alors Di est pr\u00e9f\u00e9r\u00e9 \u00e0 Dj pour la requ\u00eate Q."
        },
        {
            "start": 2552,
            "end": 2554,
            "text": "Et puis, etc etc."
        },
        {
            "start": 2554,
            "end": 2560,
            "text": "On peut mettre en place \u00e9norm\u00e9ment de strat\u00e9gies comme \u00e7a pour construire des ordres de pertinence entre documents."
        },
        {
            "start": 2560,
            "end": 2573,
            "text": "Voil\u00e0, alors il faut \u00eatre quand m\u00eame vigilant avec \u00e7a, et avec l'exploitation des clics comme substitut de pertinence, parce que c'est fortement biais\u00e9."
        },
        {
            "start": 2573,
            "end": 2586,
            "text": "Voil\u00e0, typiquement prenons un exemple, avec la requ\u00eate \u00e0 Implicit Feedback in Information Retrieval, l'utilisateur clique sur le premier document."
        },
        {
            "start": 2586,
            "end": 2594,
            "text": "Est-ce que pour autant on peut dire que le premier document est plus pertinent que le deuxi\u00e8me pour ce besoin d'information ?"
        },
        {
            "start": 2594,
            "end": 2596,
            "text": "C'est tr\u00e8s discutable."
        },
        {
            "start": 2596,
            "end": 2606,
            "text": "Voil\u00e0, de m\u00eame, un utilisateur ne clique pas sur un des premiers documents."
        },
        {
            "start": 2606,
            "end": 2610,
            "text": "Est-ce qu'on peut pour autant dire que le deuxi\u00e8me est beaucoup plus pertinent que le premier ?"
        },
        {
            "start": 2610,
            "end": 2612,
            "text": "C'est pareil, c'est tr\u00e8s discutable."
        },
        {
            "start": 2612,
            "end": 2622,
            "text": "Et c'est des choix d'interpr\u00e9tation des clics, qui sont des choix de conception."
        },
        {
            "start": 2622,
            "end": 2629,
            "text": "Le clic, \u00e7a peut \u00eatre aussi un \u00e9l\u00e9ment de pertinence tr\u00e8s brut\u00e9."
        },
        {
            "start": 2629,
            "end": 2647,
            "text": "Par exemple, ici, j'ai toujours la m\u00eame requ\u00eate, Implicit Feedback in Information Retrieval, mais parce que mon moteur de recherche n'est pas tr\u00e8s bon, il me renvoie cette liste d'images, je me demande pourquoi, \u00e7a m'interroge, \u00e7a attire ma curiosit\u00e9, je clique."
        },
        {
            "start": 2647,
            "end": 2657,
            "text": "Prenons aussi un autre cas qui est difficile \u00e0 interpr\u00e9ter, o\u00f9 cette fois-ci, ma requ\u00eate, c'est la capitale de Londres."
        },
        {
            "start": 2657,
            "end": 2676,
            "text": "Donc, je vais me demander pourquoi, je vais me demander si je peux me faire un lien avec ce document, je vais me demander si je peux me faire un lien avec ce document, je vais me demander si je peux me faire un lien avec ce document, je vais me demander si je peux me faire un lien avec ce document, o\u00f9 cette fois-ci, ma requ\u00eate, c'est la capitale de Londres."
        },
        {
            "start": 2676,
            "end": 2681,
            "text": "L'utilisateur, fois ces r\u00e9sultats-l\u00e0, il quitte son aviateur."
        },
        {
            "start": 2681,
            "end": 2689,
            "text": "On pourrait interpr\u00e9ter \u00e7a comme, il n'est pas satisfait des r\u00e9sultats du moteur de recherche."
        },
        {
            "start": 2689,
            "end": 2700,
            "text": "Le probl\u00e8me, c'est qu'il va exactement avoir le m\u00eame comportement pour cette requ\u00eate-l\u00e0 et ces r\u00e9sultats-l\u00e0."
        },
        {
            "start": 2700,
            "end": 2700,
            "text": "Pourquoi ?"
        },
        {
            "start": 2700,
            "end": 2703,
            "text": "Parce qu'il a directement la r\u00e9ponse \u00e0 sa question."
        },
        {
            "start": 2703,
            "end": 2717,
            "text": "Et donc, on voit bien toute la difficult\u00e9, finalement, qui est li\u00e9e \u00e0 l'analyse des clics et notamment \u00e0 l'analyse des clics comme substitut de cette notion de pertinence."
        },
        {
            "start": 2717,
            "end": 2736,
            "text": "C'est juste des petits exemples pour vous montrer \u00e0 quel point tout cela est forc\u00e9ment bruit\u00e9 et \u00e0 quel point \u00e7a peut introduire des biais et m\u00eame des erreurs dans l'exploitation des clics, justement pour l'am\u00e9lioration des moteurs de recherche."
        },
        {
            "start": 2736,
            "end": 2748,
            "text": "Voil\u00e0, juste pour en finir sur les jeux de donn\u00e9es, une rapide pr\u00e9sentation du jeu de donn\u00e9es Lettor, qui est un petit peu le jeu de donn\u00e9es qui est devenu tr\u00e8s standard pour l'\u00e9valuation des algorithmes d'ordonnancement."
        },
        {
            "start": 2748,
            "end": 2757,
            "text": "C'est un jeu de donn\u00e9es qui a \u00e9t\u00e9 propos\u00e9 par Microsoft Research avec un certain nombre de collections propos\u00e9es dans le jeu de donn\u00e9es."
        },
        {
            "start": 2757,
            "end": 2772,
            "text": "Et puis, il y a eu \u00e9norm\u00e9ment d'autres initiatives, notamment Yahoo qui a propos\u00e9 un challenge autour de l'apprentissage pour l'ordonnancement, et puis ensuite Microsoft, avec des jeux de donn\u00e9es \u00e0 chaque fois de plus en plus gros."
        },
        {
            "start": 2772,
            "end": 2796,
            "text": "Voil\u00e0, donc juste des ordres de grandeur, donc Lettor, on a une dizaine de jeux de donn\u00e9es avec \u00e0 chaque fois un certain nombre de requ\u00eates et pour chaque requ\u00eate, un certain nombre de documents associ\u00e9s, soit exprim\u00e9s sous paires, soit donn\u00e9s comme une liste de documents pertinents ordonn\u00e9s."
        },
        {
            "start": 2796,
            "end": 2801,
            "text": "Et puis aussi un certain nombre de variables descriptives des donn\u00e9es."
        },
        {
            "start": 2801,
            "end": 2828,
            "text": "Voil\u00e0, typiquement dans Lettor, on a chaque paire de donn\u00e9es requ\u00eates documents et d\u00e9finies selon un certain nombre de variables, dont ces 37 l\u00e0, donc qui sont typiquement les caract\u00e9ristiques qu'on utilise, que vous avez essay\u00e9 d'utiliser dans le Lab 1 pour caract\u00e9riser aussi vos donn\u00e9es d'apprentissage."
        },
        {
            "start": 2828,
            "end": 2856,
            "text": "Alors, il y a un probl\u00e8me dont on n'a pas vraiment trop parl\u00e9 jusqu'\u00e0 maintenant, qui est un probl\u00e8me de r\u00e9duction de la dimension et qui est nettement li\u00e9 au fait que l'ensemble de nos vecteurs caract\u00e9ristiques sont d\u00e9crits dans un espace de tr\u00e8s grande dimension, un espace qui est engendr\u00e9 par le vocabulaire de termes, donc avec autant de termes dans le vocabulaire en fait que de dimensions dans l'espace de repr\u00e9sentation."
        },
        {
            "start": 2856,
            "end": 2865,
            "text": "Et donc, il peut se poser un certain nombre de questions pour essayer de r\u00e9duire cet espace de repr\u00e9sentation."
        },
        {
            "start": 2865,
            "end": 2870,
            "text": "Et notamment, on peut faire \u00e7a en utilisant \u00e9norm\u00e9ment de techniques de s\u00e9lection de variables."
        },
        {
            "start": 2870,
            "end": 2876,
            "text": "Donc l'objectif, c'est de r\u00e9duire la grande dimensionnalit\u00e9 de l'espace de repr\u00e9sentation des documents."
        },
        {
            "start": 2876,
            "end": 2891,
            "text": "Avec plusieurs approches, il y a des approches qui consistent \u00e0 faire du seuil sur la mesure des f, o\u00f9 typiquement on va supprimer les termes qui ont une faible fr\u00e9quence dont on peut penser qu'ils ne sont pas informatifs pour une classe de donn\u00e9es."
        },
        {
            "start": 2891,
            "end": 2896,
            "text": "Donc \u00e7a, c'est des choses qu'on va beaucoup utiliser quand on est dans un cadre, par exemple, de cat\u00e9gorisation de documents."
        },
        {
            "start": 2896,
            "end": 2914,
            "text": "Et puis, il y a d'autres mesures qui se basent plut\u00f4t sur des mesures d'estimation de l'information, avec notamment la mesure d'information mutuelle ponctuelle, qui consiste \u00e0 estimer l'information que la pr\u00e9sence d'un terme apporte \u00e0 la classe C et apporte \u00e0 la cat\u00e9gorisation de cette classe."
        },
        {
            "start": 2914,
            "end": 2923,
            "text": "Et puis, l'information mutuelle qui s'int\u00e9resse \u00e0 la pr\u00e9sence et l'absence de termes et ce qu'ils apportent sur la cat\u00e9gorisation de la classe."
        },
        {
            "start": 2923,
            "end": 2934,
            "text": "Donc, je ne d\u00e9taillerai pas ces approches ici, mais vous avez une version d\u00e9taill\u00e9e des slides sur Edunao."
        },
        {
            "start": 2934,
            "end": 2946,
            "text": "Voil\u00e0, et j'aimerais juste terminer ce cours par une petite ouverture et justement, qui traite de cet aspect relatif \u00e0 la repr\u00e9sentation de l'information."
        },
        {
            "start": 2946,
            "end": 2959,
            "text": "Donc, on a utilis\u00e9 jusqu'\u00e0 maintenant des repr\u00e9sentations vectorielles et donc sur un espace qui est engendr\u00e9 par le vocabulaire de termes, avec comme dimension la cardinalit\u00e9 du vocabulaire de termes."
        },
        {
            "start": 2959,
            "end": 2969,
            "text": "Donc, voil\u00e0, on peut repr\u00e9senter le document de mani\u00e8re binaire en termes de pr\u00e9sence ou absence des termes de ce vocabulaire."
        },
        {
            "start": 2969,
            "end": 2979,
            "text": "On peut aussi repr\u00e9senter un terme de cette mani\u00e8re-l\u00e0, c'est-\u00e0-dire que le terme sur l'espace engendr\u00e9 par le vocabulaire de termes, il a une repr\u00e9sentation de l'espace."
        },
        {
            "start": 2979,
            "end": 2987,
            "text": "Il a une repr\u00e9sentation one-hot, donc il y a 1 pour la dimension qui correspond au terme et 0 pour les autres dimensions."
        },
        {
            "start": 2987,
            "end": 2998,
            "text": "Et puis, bien s\u00fbr, tout \u00e7a, on peut le repr\u00e9senter avec de la pond\u00e9ration, en avis \u00e0 quel point la pond\u00e9ration \u00e9tait importante en recherche d'informations."
        },
        {
            "start": 2998,
            "end": 3013,
            "text": "Voil\u00e0, alors il y a des grosses limitations \u00e0 ce type de repr\u00e9sentation et notamment, en fait, tr\u00e8s li\u00e9es \u00e0 la compr\u00e9hension, l'\u00e9tude fine de la langue et ce qu'on pourrait en faire en recherche d'informations."
        },
        {
            "start": 3013,
            "end": 3023,
            "text": "Et notamment, il est tr\u00e8s difficile de traiter des probl\u00e8mes de polys\u00e9mie et de synonymie avec ce type de repr\u00e9sentation."
        },
        {
            "start": 3023,
            "end": 3031,
            "text": "Ce type de repr\u00e9sentation, elles font toujours l'hypoth\u00e8se que les mots sont ind\u00e9pendants, alors on sait bien que les mots ne sont pas ind\u00e9pendants."
        },
        {
            "start": 3031,
            "end": 3040,
            "text": "Et puis, finalement, ce ne sont pas des repr\u00e9sentations qui arrivent \u00e0 capter r\u00e9ellement la s\u00e9mantique d'un mot."
        },
        {
            "start": 3040,
            "end": 3055,
            "text": "Voil\u00e0, et donc, on pourrait vouloir essayer d'am\u00e9liorer ces repr\u00e9sentations des mots et donc des documents en essayant de trouver des repr\u00e9sentations qui portent un peu plus la s\u00e9mantique des mots."
        },
        {
            "start": 3055,
            "end": 3059,
            "text": "Par exemple, pour faire de l'expansion de requ\u00eates en prenant en compte cette s\u00e9mantique."
        },
        {
            "start": 3059,
            "end": 3067,
            "text": "Supposons que vous cherchiez \u00e0 faire une requ\u00eate jaguar, est-ce qu'on parle de jaguar la voiture, est-ce qu'on parle de jaguar animal ?"
        },
        {
            "start": 3067,
            "end": 3076,
            "text": "Voil\u00e0, c'est typiquement ce type de s\u00e9mantique qu'on aimerait capter et associer aux mots."
        },
        {
            "start": 3076,
            "end": 3087,
            "text": "Donc, l'expansion de requ\u00eates, c'est une technique de recherche d'informations qui a principalement comme objectif d'augmenter le rappel des syst\u00e8mes de recherche."
        },
        {
            "start": 3087,
            "end": 3094,
            "text": "On va typiquement \u00e9tendre la requ\u00eate en utilisant un certain nombre de ressources globales qui sont non-d\u00e9pendantes de la requ\u00eate."
        },
        {
            "start": 3094,
            "end": 3118,
            "text": "Alors, il y a trois approches principales, des techniques d'expansion de requ\u00eates avec des th\u00e9saurus existants, donc par exemple WordNet ou PuMed, une expansion avec une g\u00e9n\u00e9ration automatique de th\u00e9saurus, et puis il y a aussi, bien s\u00fbr, des expansions de requ\u00eates de l'ordre de la correction orthographique dont on ne parlera pas ici."
        },
        {
            "start": 3118,
            "end": 3128,
            "text": "Voil\u00e0, donc le principe, c'est que chaque terme de la requ\u00eate va pouvoir \u00eatre \u00e9tendu avec des synonymes ou des termes qui sont s\u00e9mantiquement proches en utilisant un th\u00e9saurus."
        },
        {
            "start": 3128,
            "end": 3135,
            "text": "Donc, on va voir justement comment on peut capter ces termes s\u00e9mantiquement proches avec donc diff\u00e9rentes approches."
        },
        {
            "start": 3135,
            "end": 3139,
            "text": "Donc, la notion de th\u00e9saurus, elle est multiple."
        },
        {
            "start": 3139,
            "end": 3148,
            "text": "Soit on a des th\u00e9saurus manuels qui sont maintenus par des th\u00e9saurus, soit on a des th\u00e9saurus manuels qui sont maintenus par des \u00e9diteurs, comme par exemple PuMed ou WordNet."
        },
        {
            "start": 3148,
            "end": 3156,
            "text": "Soit on peut d\u00e9river automatiquement des th\u00e9saurus \u00e0 partir notamment de statistiques de co-occurrence entre mots sur une collection."
        },
        {
            "start": 3156,
            "end": 3159,
            "text": "Donc, on prend en compte aussi le contexte des mots dans une collection."
        },
        {
            "start": 3159,
            "end": 3167,
            "text": "Et on peut aussi, bien s\u00fbr, utiliser l'analyse des logs des requ\u00eates et en faisant de la reformulation de requ\u00eates en utilisant les requ\u00eates qui ont \u00e9t\u00e9 formul\u00e9es par d'autres utilisateurs."
        },
        {
            "start": 3167,
            "end": 3205,
            "text": "Donc, si je pars d'un th\u00e9saurus existant, automatiquement, ce qui est fait dans le site PuMed, qui est un site qui permet d'acc\u00e9der \u00e0 des articles m\u00e9dicaux, ou voil\u00e0, si je fais une requ\u00eate cancer, automatiquement, ma requ\u00eate va \u00eatre \u00e9tendue avec les termes n\u00e9oplasme et cancer, qui sont en fait des mots d'une ressource terminologique qui s'appelle MeSH, et qui sont donc des termes qui sont beaucoup plus m\u00e9dicaux."
        },
        {
            "start": 3205,
            "end": 3206,
            "text": "Je peux aussi utiliser WordNet."
        },
        {
            "start": 3206,
            "end": 3207,
            "text": "Alors, WordNet, c'est quoi ?"
        },
        {
            "start": 3207,
            "end": 3215,
            "text": "C'est une ressource lexicale, qui est certainement la ressource lexicale qui est la plus utilis\u00e9e en traitement du langage naturel."
        },
        {
            "start": 3215,
            "end": 3218,
            "text": "Elle est d\u00e9velopp\u00e9e \u00e0 Princeton depuis 1985."
        },
        {
            "start": 3218,
            "end": 3228,
            "text": "Et en gros, elle est constitu\u00e9e de ce qu'on appelle un ensemble de synsets, qui est en fait un groupe de sens ou des synonymes."
        },
        {
            "start": 3228,
            "end": 3234,
            "text": "Et ces synsets, ils sont reli\u00e9s entre eux par des relations lexicales et s\u00e9mantico-conceptuelles."
        },
        {
            "start": 3234,
            "end": 3240,
            "text": "Donc, c'est une vraie grosse ressource lexicale qui est int\u00e9ressante et qui a \u00e9t\u00e9 port\u00e9e en plusieurs langues."
        },
        {
            "start": 3240,
            "end": 3244,
            "text": "Et elle a aussi l'avantage d'\u00eatre disponible dans NLTK."
        },
        {
            "start": 3244,
            "end": 3250,
            "text": "Donc voil\u00e0, par exemple, prenons une requ\u00eate dog."
        },
        {
            "start": 3250,
            "end": 3255,
            "text": "Voil\u00e0, j'importe NLTK et j'importe WordNet."
        },
        {
            "start": 3255,
            "end": 3262,
            "text": "Et je demande finalement d'acc\u00e9der \u00e0 l'ensemble des synsets qui sont associ\u00e9s au terme dog."
        },
        {
            "start": 3262,
            "end": 3268,
            "text": "Voil\u00e0, on voit qu'il y en a un certain nombre de synsets qui sont associ\u00e9s au terme dog, dont tous ceux-l\u00e0."
        },
        {
            "start": 3268,
            "end": 3271,
            "text": "Et on voit qu'on a une d\u00e9finition associ\u00e9e."
        },
        {
            "start": 3271,
            "end": 3277,
            "text": "Et donc, du coup, je peux tout \u00e0 fait utiliser cette d\u00e9finition pour faire de l'expansion de requ\u00eate."
        },
        {
            "start": 3277,
            "end": 3292,
            "text": "Typiquement, je peux aussi prendre un ordre dans les synsets qui me sont renvoy\u00e9s, qui sont des ordres de fr\u00e9quence, dans ce m\u00e9canisme d'expansion de requ\u00eate."
        },
        {
            "start": 3292,
            "end": 3300,
            "text": "Je peux aussi vouloir cr\u00e9er mon propre th\u00e9saurus de co-occurrence en faisant de l'analyse de la distribution des mots dans un document."
        },
        {
            "start": 3300,
            "end": 3308,
            "text": "Typiquement, \u00e7a se base sur une similarit\u00e9 des mots qui est beaucoup plus contextuelle que r\u00e9ellement s\u00e9mantique."
        },
        {
            "start": 3308,
            "end": 3314,
            "text": "C'est-\u00e0-dire qu'on va prendre en compte une similarit\u00e9, on va consid\u00e9rer que deux mots sont similaires s'ils co-occurrent avec les m\u00eames mots."
        },
        {
            "start": 3314,
            "end": 3333,
            "text": "Typiquement, voiture et moto sont similaires parce qu'ils vont co-occurrer tr\u00e8s souvent dans un corpus avec les mots route et essence, qui n'est pas tout \u00e0 fait la m\u00eame chose que la similarit\u00e9 quand ils co-occurrent avec la m\u00eame relation grammaticale."
        },
        {
            "start": 3333,
            "end": 3348,
            "text": "Et donc, ce th\u00e9ausaurus de co-occurrence, en fait, on peut le construire avec des outils dont on a d\u00e9j\u00e0 parl\u00e9, notamment \u00e0 partir de la matrice d'incidence terme document, ou WIJ, c'est le poids normalis\u00e9 pour le terme TI dans le document DJI."
        },
        {
            "start": 3348,
            "end": 3367,
            "text": "Si je construis la matrice de co-occurrence C en faisant A fois la transform\u00e9e de A, eh bien, j'obtiens donc ma matrice de co-occurrence et pour chaque terme TI, je peux prendre les termes qui sont maximum dans C, bien s\u00fbr en utilisant ce que j'obtiens avec les points."
        },
        {
            "start": 3367,
            "end": 3379,
            "text": "Et voil\u00e0, par exemple, ce que je peux obtenir sur un exemple donn\u00e9, qui sont donc les termes qui co-occurrent avec un certain nombre de mots donn\u00e9s en utilisant cette approche."
        },
        {
            "start": 3379,
            "end": 3387,
            "text": "Et typiquement, je pourrais du coup faire de l'expansion de requ\u00eates en utilisant ces plus proches voisins dans mon th\u00e9aurus de co-occurrence."
        },
        {
            "start": 3387,
            "end": 3392,
            "text": "Par exemple, lithogra, je pourrais l'\u00e9tendre avec drawing, Picasso, Dali, etc."
        },
        {
            "start": 3392,
            "end": 3398,
            "text": "On voit que du coup, avec cette approche, la qualit\u00e9 des associations est souvent discutable."
        },
        {
            "start": 3398,
            "end": 3410,
            "text": "En plus de \u00e7a, on a souvent une matrice qui est fortement creuse, on avait d\u00e9j\u00e0 le probl\u00e8me avec la matrice terme incidence, et du coup, l'imbigu\u00eft\u00e9 des termes peut quand m\u00eame introduire de mauvaises associations entre nos termes."
        },
        {
            "start": 3410,
            "end": 3421,
            "text": "Et donc, on va essayer de trouver une nouvelle approche, et notamment, l'approche, elle va consister \u00e0 essayer d'apprendre directement les relations entre les termes."
        },
        {
            "start": 3421,
            "end": 3425,
            "text": "Donc, revenons \u00e0 notre probl\u00e8me de la relation entre termes."
        },
        {
            "start": 3425,
            "end": 3427,
            "text": "La repr\u00e9sentation des termes."
        },
        {
            "start": 3427,
            "end": 3430,
            "text": "Nos termes, ils sont consid\u00e9r\u00e9s comme des symboles atomiques."
        },
        {
            "start": 3430,
            "end": 3437,
            "text": "Chaque terme, c'est une dimension, et donc, on l'a vu avec une repr\u00e9sentation de type ouanote."
        },
        {
            "start": 3437,
            "end": 3448,
            "text": "Voil\u00e0, prenons par exemple le terme motel et le terme h\u00f4tel, et leur repr\u00e9sentation ouanote."
        },
        {
            "start": 3448,
            "end": 3462,
            "text": "Supposons que notre requ\u00eate contienne le terme motel, et que les documents qui sont dans notre corpus ne parlent que de termes h\u00f4tel."
        },
        {
            "start": 3462,
            "end": 3483,
            "text": "Et bien, typiquement, c'est l'inverse ici, mais si j'ai une requ\u00eate sur h\u00f4tel et que le document parle de motel, alors avec les principes de recherche qu'on a mis en \u0153uvre, qui font appel, du coup, au produit scalaire, je vais forc\u00e9ment avoir une similarit\u00e9 qui va \u00eatre nulle entre ma requ\u00eate et mon document."
        },
        {
            "start": 3483,
            "end": 3492,
            "text": "Donc, c'est vraiment ce probl\u00e8me-l\u00e0, en fait, qui est limitant et pour lequel on aimerait apporter une solution."
        },
        {
            "start": 3492,
            "end": 3497,
            "text": "Et typiquement, ce qu'on aimerait, c'est avoir une meilleure repr\u00e9sentation de nos termes."
        },
        {
            "start": 3497,
            "end": 3514,
            "text": "Donc, l'id\u00e9e, \u00e7a va \u00eatre d'apprendre une repr\u00e9sentation qui va \u00eatre de plus petite dimension d'un mot donn\u00e9, d'un terme, donc dans RD, et telle que cette repr\u00e9sentation U, elle soit telle que UTV repr\u00e9sente la similarit\u00e9 entre mots."
        },
        {
            "start": 3514,
            "end": 3521,
            "text": "Donc, le principe, \u00e7a va \u00eatre de repr\u00e9senter chaque mot par ce qu'on appelle un vecteur distributionnel."
        },
        {
            "start": 3521,
            "end": 3523,
            "text": "Et donc, en gros, on va repr\u00e9senter un mot \u00e0 l'aide de ses voisins."
        },
        {
            "start": 3523,
            "end": 3526,
            "text": "C'est pour \u00e7a qu'on appelle \u00e7a un vecteur distributionnel."
        },
        {
            "start": 3526,
            "end": 3541,
            "text": "Et donc, \u00e7a, \u00e7a utilise un principe qui est un principe qui date des ann\u00e9es 50, qui a \u00e9t\u00e9 \u00e9nonc\u00e9 par First, et qui dit qu'on peut conna\u00eetre la compagnie, qu'on peut conna\u00eetre, pardon, qu'on peut conna\u00eetre un mot par la compagnie de ce mot-l\u00e0."
        },
        {
            "start": 3541,
            "end": 3548,
            "text": "Voil\u00e0, donc on va aussi, du coup, mettre en place dans ce m\u00e9canisme de similarit\u00e9 distributionnelle de la similarit\u00e9 entre vecteurs."
        },
        {
            "start": 3548,
            "end": 3558,
            "text": "Et donc, l'hypoth\u00e8se, c'est que le sens d'un mot inconnu, il va \u00eatre devinable par son contexte, et que le contexte va bien s\u00fbr aider \u00e0 caract\u00e9riser le sens du mot."
        },
        {
            "start": 3558,
            "end": 3562,
            "text": "Et puis, la similarit\u00e9 de contexte va aider \u00e0 caract\u00e9riser la similarit\u00e9 de sens."
        },
        {
            "start": 3562,
            "end": 3568,
            "text": "Voil\u00e0, donc \u00e7a, c'est ce qu'on appelle la similarit\u00e9 distributionnelle, qui consiste \u00e0 repr\u00e9senter un mot par ses voisins."
        },
        {
            "start": 3568,
            "end": 3571,
            "text": "Donc typiquement, l'id\u00e9e, c'est \u00e7a."
        },
        {
            "start": 3571,
            "end": 3579,
            "text": "Si je veux caract\u00e9riser banking, eh bien, je veux utiliser les voisins de banking pour caract\u00e9riser justement le mot banking."
        },
        {
            "start": 3579,
            "end": 3586,
            "text": "Et inversement, je peux caract\u00e9riser le contexte d'un mot \u00e0 partir du mot du m\u00eame."
        },
        {
            "start": 3586,
            "end": 3590,
            "text": "\u00c7a, c'est vraiment une des id\u00e9es cl\u00e9s pour les mots de la compagnie."
        },
        {
            "start": 3590,
            "end": 3610,
            "text": "\u00c7a, c'est vraiment une des id\u00e9es cl\u00e9s du traitement du langage naturel moderne, et c'est notamment ce qu'on appelle les techniques d'embedding de mots, type Word2Vec, GloVe, etc., et puis les mod\u00e8les de langues plus r\u00e9cents, pr\u00e9entra\u00een\u00e9es, de type BERT, par exemple."
        },
        {
            "start": 3610,
            "end": 3614,
            "text": "Voil\u00e0, alors la question, c'est comment repr\u00e9senter un mot par ses voisins."
        },
        {
            "start": 3614,
            "end": 3618,
            "text": "Et donc, pour \u00e7a, on va utiliser une repr\u00e9sentation dense de plus petite dimension."
        },
        {
            "start": 3618,
            "end": 3630,
            "text": "L'id\u00e9e, c'est que le nombre de sujets couverts dans un corpus, il est petit, et l'id\u00e9e, \u00e7a va \u00eatre de stocker l'information la plus importante dans ce petit nombre de dimensions."
        },
        {
            "start": 3630,
            "end": 3634,
            "text": "Donc, on retombe sur un probl\u00e8me de r\u00e9duction de la dimension."
        },
        {
            "start": 3634,
            "end": 3637,
            "text": "Alors, comment on peut repr\u00e9senter un mot par ses voisins ?"
        },
        {
            "start": 3637,
            "end": 3641,
            "text": "L'approche classique, c'est d'utiliser une matrice de co-occurrence."
        },
        {
            "start": 3641,
            "end": 3642,
            "text": "On l'a parl\u00e9 juste avant."
        },
        {
            "start": 3642,
            "end": 3644,
            "text": "Apr\u00e8s, il y a deux options."
        },
        {
            "start": 3644,
            "end": 3649,
            "text": "Soit je prends en compte tout le document, soit je prends une fen\u00eatre autour de chaque mot."
        },
        {
            "start": 3649,
            "end": 3656,
            "text": "Si je prends en compte tout le document, je vais tomber sur des probl\u00e8mes qui sont connus comme des probl\u00e8mes d'analyse en s\u00e9mantique latente."
        },
        {
            "start": 3656,
            "end": 3666,
            "text": "Et si je prends une fen\u00eatre autour de chaque mot, ben, je suis typiquement dans ce qu'on fait en analyse distributionnelle."
        },
        {
            "start": 3666,
            "end": 3681,
            "text": "Prenons un petit exemple avec une fen\u00eatre de taille A. Donc, j'ai un petit corpus qui est celui-l\u00e0, donc avec un ensemble de termes qui sont I like, deep learning, NLP, enjoy flying, et puis le point."
        },
        {
            "start": 3681,
            "end": 3688,
            "text": "Et puis, comme j'ai une fen\u00eatre de taille 1, ben, je vais construire du coup ma matrice de co-occurrence."
        },
        {
            "start": 3688,
            "end": 3697,
            "text": "Donc, I n'est jamais voisin de I. I est voisin de like deux fois."
        },
        {
            "start": 3697,
            "end": 3699,
            "text": "I like, I like."
        },
        {
            "start": 3699,
            "end": 3703,
            "text": "I est voisin de enjoy une fois."
        },
        {
            "start": 3703,
            "end": 3705,
            "text": "I n'est jamais voisin de deep, etc."
        },
        {
            "start": 3705,
            "end": 3713,
            "text": "Vous avez compris comment on construit cette matrice de co-occurrence avec un voisinage de taille 1."
        },
        {
            "start": 3713,
            "end": 3721,
            "text": "Donc, j'ai comme \u00e7a la repr\u00e9sentation d'un mot par ses voisins, donc avec cette matrice de co-occurrence."
        },
        {
            "start": 3721,
            "end": 3732,
            "text": "Apr\u00e8s, on peut se poser des questions sur cette repr\u00e9sentation, notamment sur son passage \u00e0 l'\u00e9chelle, parce qu'on va avoir une taille de la matrice qui va cro\u00eetre avec le vocabulaire."
        },
        {
            "start": 3732,
            "end": 3743,
            "text": "J'ai une matrice qui est non dense, et donc l'id\u00e9e, \u00e7a va \u00eatre d'essayer de stocker l'information qui est contenue dans cette matrice dans un nombre fix\u00e9 plus petit de dimensions."
        },
        {
            "start": 3743,
            "end": 3748,
            "text": "Notamment, on estime qu'entre 25 et 1000... 1000 dimensions, c'est tr\u00e8s bien."
        },
        {
            "start": 3748,
            "end": 3754,
            "text": "Et donc on va essayer d'utiliser sur cette matrice de co-occurrence des techniques de r\u00e9duction de la dimension."
        },
        {
            "start": 3754,
            "end": 3763,
            "text": "Alors, une technique de r\u00e9duction de la dimension qui est assez ad\u00e9quate pour ce probl\u00e8me, c'est la d\u00e9composition en valeur singuli\u00e8re."
        },
        {
            "start": 3763,
            "end": 3771,
            "text": "On sait que toute matrice A de dimension m fois n, avec m qui est sup\u00e9rieur \u00e0 n, on peut la d\u00e9composer de la mani\u00e8re suivante."
        },
        {
            "start": 3771,
            "end": 3790,
            "text": "A, c'est U, sigma, transpos\u00e9 de V, avec U qui est de dimension m fois m unitaire, telle que U, T, U c'est la matrice identit\u00e9, sigma qui est une matrice m fois n diagonale avec des coefficients r\u00e9els positifs, donc les sigma 1 sup\u00e9rieur \u00e0 sigma 2, etc."
        },
        {
            "start": 3790,
            "end": 3794,
            "text": "Et puis V qui est de dimension n fois n et qui est aussi unitaire."
        },
        {
            "start": 3794,
            "end": 3805,
            "text": "Et puis les sigma i, en fait, ce sont ce qu'on appelle les valeurs singuli\u00e8res de la matrice A. Voil\u00e0, donc une illustration de cette d\u00e9composition en valeur singuli\u00e8re."
        },
        {
            "start": 3805,
            "end": 3846,
            "text": "Pour une matrice X qui est ici de taille V fois V, cardinal de V \u00e9tant la taille de notre vocabulaire, j'ai cette d\u00e9composition avec cette premi\u00e8re matrice U de taille cardinal V fois cette matrice diagonale, avec les sigma 1 qui vont \u00eatre les valeurs singuli\u00e8res, et puis donc la matrice V. Donc les valeurs singuli\u00e8res de X sont les racines carr\u00e9es des valeurs propres de X, T, X. Les colonnes de V, les valeurs singuli\u00e8res droits de X, c'est les vecteurs propres de X, T, X."
        },
        {
            "start": 3846,
            "end": 3866,
            "text": "Et les colonnes de U, c'est les vecteurs propres de X, X, T. Voil\u00e0, cette d\u00e9composition en valeur singuli\u00e8re, on peut l'utiliser pour r\u00e9duire la dimension en s\u00e9lectionnant uniquement les cas premiers vecteurs singuliers."
        },
        {
            "start": 3866,
            "end": 3908,
            "text": "Je me contente ici de s\u00e9lectionner les cas premiers vecteurs singuliers, donc je tronque ma matrice U et ma matrice V. Voil\u00e0, donc la r\u00e9duction de la dimension avec la SVD, c'est \u00e9tant donn\u00e9 un vocabulaire de terme V, je g\u00e9n\u00e8re la matrice X de taille cardinal V fois cardinal V, j'applique la SVD pour obtenir U, S et V, et puis je choisis les cas premi\u00e8res colonnes de U pour avoir des vecteurs de mots de dimension K. Et \u00e7a, du coup, ces cas premi\u00e8res colonnes de U, c'est ce qu'on peut appeler des repr\u00e9sentations distributionnelles de m\u00e9moire."
        },
        {
            "start": 3908,
            "end": 3921,
            "text": "Et puis on a aussi des informations statistiques qui indiquent comment le fait de choisir que les cas premi\u00e8res dimensions capturent ou pas de l'information."
        },
        {
            "start": 3921,
            "end": 3929,
            "text": "Voil\u00e0, reprenons notre petit exemple de tout \u00e0 l'heure et donc appliquons ce principe de r\u00e9duction de la dimension sur ce petit exemple-l\u00e0."
        },
        {
            "start": 3929,
            "end": 3951,
            "text": "Donc j'ai mon ensemble de mots qui est celui-l\u00e0, je fais appel au module linale de NumPy, et puis j'applique la SVD sur cette matrice X qui est donc ma matrice de co-occurrence telle que construite tout \u00e0 l'heure, avec un voisinage de contexte, une fen\u00eatre de contexte de taille 1."
        },
        {
            "start": 3951,
            "end": 3976,
            "text": "Je vais du coup, apr\u00e8s, s\u00e9lectionner les deux premi\u00e8res colonnes de U, donc les deux plus grandes valeurs singuli\u00e8res, ce que je fais avec ce code-l\u00e0, et puis j'affiche du coup les mots obtenus, les repr\u00e9sentations des mots obtenus, donc dans cet espace \u00e0 deux dimensions."
        },
        {
            "start": 3976,
            "end": 3991,
            "text": "Et donc on voit que c'est int\u00e9ressant, que j'ai peut-\u00eatre effectivement capt\u00e9 une information un peu plus s\u00e9mantique avec ce type de repr\u00e9sentation, et notamment le fait que NLP soit proche de DEEP dans cet espace, c'est relativement pertinent."
        },
        {
            "start": 3991,
            "end": 3999,
            "text": "ENJOY et LIKE sont pas tr\u00e8s loin non plus dans cet espace."
        },
        {
            "start": 3999,
            "end": 4009,
            "text": "Donc c'est int\u00e9ressant, mais c'est bien s\u00fbr co\u00fbteux en termes de calcul, et puis il est difficile d'incorporer de nouveaux mots, de nouveaux documents."
        },
        {
            "start": 4009,
            "end": 4015,
            "text": "Juste ce qu'il faut retenir de cette approche, c'est qu'on va souhaiter repr\u00e9senter un mot par un vecteur dense."
        },
        {
            "start": 4015,
            "end": 4023,
            "text": "Voil\u00e0, typiquement ce qu'on souhaite c'est un mot associ\u00e9 \u00e0 ce type de vecteur l\u00e0, de petite dimension."
        },
        {
            "start": 4023,
            "end": 4032,
            "text": "Et donc cette repr\u00e9sentation, en fait on peut la prendre, et notamment on peut la prendre avec des approches qu'utilisent des r\u00e9seaux de neurones profonds."
        },
        {
            "start": 4032,
            "end": 4048,
            "text": "Donc c'est vraiment cette id\u00e9e-l\u00e0, apprendre directement la repr\u00e9sentation dans ce d\u00e9mo, et \u00e7a c'est typiquement ce qui a \u00e9t\u00e9 propos\u00e9 par Mikolov en 2013 avec l'approche Word2Vec, o\u00f9 l'id\u00e9e c'\u00e9tait de pr\u00e9dire le contexte des mots directement plut\u00f4t que de compter les co-occurrences."
        },
        {
            "start": 4048,
            "end": 4061,
            "text": "Donc apr\u00e8s il y a eu une famille \u00e9norme d'algorithmes qui ont suivi, donc GloVe, BERT, qui est des mod\u00e8les de langues un peu plus sophistiqu\u00e9s, mais il y a eu \u00e9norm\u00e9ment d'extensions de cette id\u00e9e."
        },
        {
            "start": 4061,
            "end": 4066,
            "text": "Donc si je reprends Word2Vec, il y a principalement deux algorithmes."
        },
        {
            "start": 4066,
            "end": 4080,
            "text": "L'algorithme qu'on appelle CEBO, donc Continuous Back of Word Model, qui pr\u00e9dit le mot cible \u00e9tant donn\u00e9 son contexte, donc \u00e9tant donn\u00e9 son voisinage, ou l'approche Skipgram qui elle, cherche \u00e0 pr\u00e9dire le contexte \u00e9tant donn\u00e9 le mot cible."
        },
        {
            "start": 4080,
            "end": 4088,
            "text": "Donc on va regarder l'approche Skipgram, donc on a des mod\u00e8les de langues, des mod\u00e8les de mots."
        },
        {
            "start": 4088,
            "end": 4110,
            "text": "Voil\u00e0, donc on a le mot banking qui est le mot central, et puis en fait je vais chercher \u00e0 pr\u00e9dire le contexte, c'est-\u00e0-dire les mots pr\u00e9c\u00e9dents et les mots suivants, justement en essayant d'estimer la probabilit\u00e9 que tel mot soit le mot avant banking, que tel mot soit le mot apr\u00e8s banking, etc."
        },
        {
            "start": 4110,
            "end": 4126,
            "text": "Donc j'ai mon mot central, j'ai donc mon fen\u00eatrage de M mots, et puis l'objectif va \u00eatre de pr\u00e9dire le contexte, c'est-\u00e0-dire les mots voisins dans cette fen\u00eatre de taille M \u00e9tant donn\u00e9 le mot cible."
        },
        {
            "start": 4126,
            "end": 4136,
            "text": "Et donc pour \u00e7a, je vais avoir une fonction objective qui va chercher \u00e0 maximiser la log probabilit\u00e9 de chaque mot de contexte \u00e9tant donn\u00e9 le mot cible."
        },
        {
            "start": 4136,
            "end": 4155,
            "text": "Donc j'ai ces probabilit\u00e9s d'un mot \u00e9tant donn\u00e9 le mot cible, d'un mot de mot contexte, avec M qui est la taille de ma fen\u00eatre, et puis je vais chercher \u00e0 maximiser \u00e7a pour l'ensemble des mots \u00e0 ma disposition, de mon corpus."
        },
        {
            "start": 4155,
            "end": 4166,
            "text": "Et donc ce que je cherche, c'est \u00e0 trouver l'ensemble des variables qui vont me permettre de maximiser \u00e7a, et donc c'est \u00e7a que je cherche \u00e0 optimiser."
        },
        {
            "start": 4166,
            "end": 4184,
            "text": "Donc ce qu'on fait g\u00e9n\u00e9ralement dans ces approches-l\u00e0, c'est qu'on va transformer une valeur qui est dans RV \u00e0 une distribution de probabilit\u00e9 au travers d'une fonction qui s'appelle une fonction softmax, et donc qui est d\u00e9finie de cette mani\u00e8re-l\u00e0."
        },
        {
            "start": 4184,
            "end": 4200,
            "text": "Et donc si j'applique \u00e7a sur ce cadre-l\u00e0, sur notre probl\u00e8me, je vais avoir une fonction softmax et donc une formulation pour notre probl\u00e8me qui va devenir celle-ci."
        },
        {
            "start": 4200,
            "end": 4206,
            "text": "Donc on voit qu'on prend bien en compte la similarit\u00e9 entre nos mots en faisant \u00e7a."
        },
        {
            "start": 4206,
            "end": 4215,
            "text": "Donc O et C ici, c'est les indices de mots de sortie et centraux, et puis U et V, c'est les U de O et V, c'est les vecteurs correspondants."
        },
        {
            "start": 4215,
            "end": 4224,
            "text": "Donc regardons plus en d\u00e9tail cette architecture d'apprentissage de repr\u00e9sentation de mots par des approches neuronales."
        },
        {
            "start": 4224,
            "end": 4236,
            "text": "Donc j'ai la repr\u00e9sentation de mon mot central, ici c'est l'entra\u00eenement syst\u00e8me, donc repr\u00e9sentation de type one-hot."
        },
        {
            "start": 4236,
            "end": 4245,
            "text": "J'ai une premi\u00e8re matrice de taille D x V. D, c'est la taille de mon vocabulaire, c'est l'esp\u00e8ce de repr\u00e9sentation de mes termes en one-hot."
        },
        {
            "start": 4245,
            "end": 4252,
            "text": "Et puis D, c'est la dimension que je souhaite pour mon vecteur dans mes digues."
        },
        {
            "start": 4252,
            "end": 4260,
            "text": "Donc avec des param\u00e8tres bien s\u00fbr que j'initialise, mais que je vais chercher \u00e0 estimer."
        },
        {
            "start": 4260,
            "end": 4277,
            "text": "Et puis en sortie de la multiplication de cette repr\u00e9sentation one-hot de mon mot et de la matrice, j'ai mon vecteur Vc, qui est mon vecteur dans mes digues que je cherche."
        },
        {
            "start": 4277,
            "end": 4288,
            "text": "Ce vecteur-l\u00e0, je vais donc ensuite le remultiplier avec mes mots de contexte."
        },
        {
            "start": 4288,
            "end": 4302,
            "text": "Voil\u00e0, donc \u00e0 nouveau j'ai une matrice de taille D x V, la taille de mon vocabulaire, donc avec ici les mots de mon contexte."
        },
        {
            "start": 4302,
            "end": 4330,
            "text": "J'en sorti le produit scalaire entre mon mot central et son mot de contexte, et je transforme ces valeurs-l\u00e0 en softmax, comme indiqu\u00e9, qui me donne une estimation de la probabilit\u00e9 de x sachant c. Du fait que le mot x soit un mot de contexte de c, et puis que je peux mettre en relation avec ma v\u00e9rit\u00e9."
        },
        {
            "start": 4330,
            "end": 4360,
            "text": "Et donc avec le principe d'optimisation des r\u00e9seaux de neurones, et notamment la descente de gradients, je vais pouvoir petit \u00e0 petit apprendre les diff\u00e9rentes points de ma matrice, et donc je vais pouvoir avoir en sortie la repr\u00e9sentation distributionnelle, donc le plongement, le mot dans mes dignes, de mon mot de contexte, donc ici."
        },
        {
            "start": 4360,
            "end": 4374,
            "text": "Alors bien s\u00fbr, il y a plein d'applications de tout \u00e7a \u00e0 l'AERI, notamment pour construire des repr\u00e9sentations des requ\u00eates et des textes selon ce formalisme, donc avec ces repr\u00e9sentations de mots."
        },
        {
            "start": 4374,
            "end": 4390,
            "text": "L'id\u00e9e c'est d'appliquer \u00e7a \u00e0 chaque mot de la requ\u00eate, chaque mot du texte, et puis apr\u00e8s avec des fonctions d'agr\u00e9gation, on obtient une repr\u00e9sentation de la requ\u00eate et une repr\u00e9sentation du texte qu'on peut apr\u00e8s mettre en correspondance avec tous les mod\u00e8les qu'on a \u00e0 notre disposition."
        },
        {
            "start": 4390,
            "end": 4394,
            "text": "Et puis il y a plein d'autres architectures qui utilisent ces m\u00e9canismes-l\u00e0."
        },
        {
            "start": 4394,
            "end": 4406,
            "text": "Une slide qui montre l'ensemble des choses qui sont possibles, l'ensemble des architectures qui sont possibles et qui sont toutes plus complexes que les unes que les autres."
        },
        {
            "start": 4406,
            "end": 4412,
            "text": "Voil\u00e0, j'en ai fini sur ce cours, donc rapide bilan sur ce qui vous a \u00e9t\u00e9 pr\u00e9sent\u00e9."
        },
        {
            "start": 4412,
            "end": 4416,
            "text": "Donc un nouveau paradigme pour l'AERI au travers de cette approche Learning to Rank."
        },
        {
            "start": 4416,
            "end": 4435,
            "text": "Donc \u00e7a, j'avais d\u00e9j\u00e0 dit la derni\u00e8re fois, des approches qui tentent d'exploiter toutes les informations \u00e0 disposition, des r\u00e9sultats comparables \u00e0 ceux des mod\u00e8les probabilistes dans le cadre de collection classique et des r\u00e9sultats bien meilleurs quand on a des grosses collections pour lesquelles on est capable de construire des espaces d'attributs tr\u00e8s riches."
        },
        {
            "start": 4435,
            "end": 4443,
            "text": "Et puis bien s\u00fbr, ce probl\u00e8me de disponibilit\u00e9 des donn\u00e9es d'entr\u00e9e, des donn\u00e9es \u00e0 noter pour mettre en place les techniques d'apprentissage."
        },
        {
            "start": 4443,
            "end": 4448,
            "text": "Voil\u00e0, des lectures conseill\u00e9es pour compl\u00e9ter ce cours."
        },
        {
            "start": 4448,
            "end": 4466,
            "text": "Et puis je vous souhaite un bon lab qui vous demandera d'appliquer l'approche par paire d'une part et d'autre part, qui vous fera travailler justement sur cette partie d'ouverture, donc sur les repr\u00e9sentations de mots par plongement lexico."
        }
    ],
    "text": "Bonjour, on attaque aujourd'hui la deuxi\u00e8me partie du cours d'apprentissage pour la recherche d'information. Je vais rapidement commencer par un r\u00eave r\u00e9capitulatif des notions introduites la derni\u00e8re fois. La derni\u00e8re fois, j'ai pr\u00e9sent\u00e9 un nouveau paradigme pour la recherche d'information qui est un paradigme bas\u00e9 sur de l'apprentissage qui s'appelle \u00ab Learning to Rank \u00bb o\u00f9 on a un jeu de donn\u00e9es d'apprentissage qui est compos\u00e9 principalement de paires requ\u00eates, documents, d\u00e9crits caract\u00e9ris\u00e9s avec un vecteur caract\u00e9ristique qu'il faut construire et puis des donn\u00e9es de sortie, des labels qui sont des valeurs de pertinence. A partir de ces donn\u00e9es d'apprentissage, on apprend une fonction qui va \u00eatre une fonction d'ordonnancement ou un crit\u00e8re et puis cette fonction d'ordonnancement apprise, on l'utilise ensuite sur des donn\u00e9es de test ou des donn\u00e9es de production qui consistent uniquement en des donn\u00e9es non labellis\u00e9es. Bien s\u00fbr, on n'a pas la v\u00e9rit\u00e9 de terrain pour ces donn\u00e9es. Et puis sur ces donn\u00e9es de test, on utilise la fonction F apprise pour pr\u00e9dire la pertinence et classer les r\u00e9sultats. En sortie, on obtient bien toujours une liste ordonn\u00e9e de documents. Ce principe d'apprentissage pour l'AERI repose sur deux concepts principaux qui sont la notation des donn\u00e9es et la repr\u00e9sentation des couples requ\u00eates-documents dans l'espace des caract\u00e9ristiques. C'est vraiment une \u00e9tape tr\u00e8s importante qu'on appelle l'\u00e9tape de description. Et puis la partie apprentissage en tant que telle avec une structure d'apprentissage classique en deux \u00e9tapes. Une \u00e9tape d'apprentissage de la fonction d'ordonnancement F, puis l'\u00e9tape de test de production sur de nouvelles requ\u00eates. On peut bien s\u00fbr aussi rajouter \u00e0 \u00e7a une \u00e9tape de validation qui, par exemple, pourrait consister \u00e0 choisir les hyperparam\u00e8tres du mod\u00e8le. La derni\u00e8re fois, il avait \u00e9t\u00e9 aussi introduit le fait qu'il existait trois grandes familles d'approches dans ce paradigm learning to rank. L'approche par points, dont on a parl\u00e9 la derni\u00e8re fois, o\u00f9 les documents sont consid\u00e9r\u00e9s de mani\u00e8re ind\u00e9pendante. L'approche par paire, dont on va parler aujourd'hui. Et l'approche par liste, qui va \u00eatre aussi rapidement \u00e9voqu\u00e9e aujourd'hui. L'approche par points, c'est une approche qui consiste \u00e0 transformer le probl\u00e8me de classement en un probl\u00e8me soit de classification, soit de r\u00e9gression, soit de r\u00e9gression ordinale. Selon le type des valeurs de pertinence qu'on a \u00e0 disposition. Typiquement, si on a des valeurs de pertinence qui sont des nombres r\u00e9els, on va se ramener \u00e0 un probl\u00e8me de r\u00e9gression. Si on a des valeurs de pertinence qui sont des cat\u00e9gories, on va se ramener \u00e0 un probl\u00e8me de classification. Et enfin, si on a des cat\u00e9gories ordonn\u00e9es, on va se ramener \u00e0 un probl\u00e8me de r\u00e9gression ordinaire. Donc voil\u00e0, on a notre vecteur caract\u00e9ristique. Trois types de probl\u00e8mes selon le type de valeur de sortie. On en d\u00e9duit la fonction f2x, on apprend la fonction f2x, qui va \u00eatre une fonction de classement. \u00c7a c'est la phase d'apprentissage. Et pour la phase de classement en tant que telle, on a nos donn\u00e9es qui sont d\u00e9crites selon le m\u00eame sch\u00e9ma de description. Et puis on va ordonner les vecteurs, les donn\u00e9es, selon le score de cette fonction f. On va aujourd'hui introduire l'approche par paire, qui est une approche tr\u00e8s d\u00e9velopp\u00e9e, tr\u00e8s utilis\u00e9e dans le paradigme learning to rank, notamment parce qu'on va pouvoir facilement cr\u00e9er des jeux de donn\u00e9es d'apprentissage, notamment pour les collections de type web. L'id\u00e9e principale de l'approche par paire, c'est de travailler sur des paires de pr\u00e9f\u00e9rences. Cette id\u00e9e, elle se base sur le fait que la notion de pertinence n'est pas une notion absolue. Plusieurs fois, tout au long du cours, on a \u00e9voqu\u00e9 cette difficult\u00e9 \u00e0 caract\u00e9riser correctement, \u00e0 d\u00e9finir correctement cette notion de pertinence, avec notamment d'ailleurs des d\u00e9finitions qui sont tr\u00e8s d\u00e9pendantes des mod\u00e8les de recherche. Binaires pour le mod\u00e8le bool\u00e9en, probabilistes pour le mod\u00e8le probabiliste, etc. Quand on a parl\u00e9 de la partie \u00e9valuation des mod\u00e8les de recherche, on a aussi \u00e9voqu\u00e9 le fait qu'il \u00e9tait difficile de juger de la pertinence d'un document, mais par contre qu'il \u00e9tait plus facile de juger de la pertinence relative de deux documents. \u00c9tant donn\u00e9 le r\u00e9sultat d'une recherche, la r\u00e9ponse d'un moteur de recherche \u00e0 un besoin d'information, pour moi utilisateur, il va \u00eatre plus facile de dire si le document N qui m'a \u00e9t\u00e9 retourn\u00e9 est plus pertinent que le document N plus 1, que de juger de la pertinence de chacun des documents. C'est vraiment cette id\u00e9e-l\u00e0 qui va \u00eatre mise en \u0153uvre dans l'approche par paire. On va travailler avec ce qu'on va appeler des jugements par pr\u00e9f\u00e9rence, et donc avec la notion de paire de pr\u00e9f\u00e9rence. Typiquement, ces paires de pr\u00e9f\u00e9rence vont exprimer le fait qu'un document va \u00eatre pr\u00e9f\u00e9r\u00e9 \u00e0 un autre pour une requ\u00eate donn\u00e9e, pour un besoin d'information donn\u00e9e. Ces jugements de pr\u00e9f\u00e9rence vont toujours \u00eatre conditionn\u00e9s par un contexte qui est la requ\u00eate qu'on a soumise au moteur de recherche. Le principe, c'est de partir sur cette id\u00e9e que les jugements par paire sont des jugements qui sont plus g\u00e9n\u00e9raux, et donc d'exprimer ce jugement comme une pr\u00e9f\u00e9rence d'un document par rapport \u00e0 un autre. \u00c9tant donn\u00e9 une requ\u00eate Q, \u00e0 chaque paire de documents DI, DJ, on va associer une valeur de pr\u00e9f\u00e9rence YIJ qui va \u00eatre \u00e0 valeur dans le moins 1. Et typiquement, on va dire que YIJ va \u00eatre \u00e9gal \u00e0 1 si XI est pr\u00e9f\u00e9r\u00e9 \u00e0 XJ pour la requ\u00eate Q. Cette relation de pr\u00e9f\u00e9rence, on va la noter de cette mani\u00e8re-l\u00e0, et donc typiquement, il faut lire cette \u00e9quation comme XI est pr\u00e9f\u00e9r\u00e9 \u00e0 XJ pour la requ\u00eate Q, toujours \u00e9tant donn\u00e9 un contexte donn\u00e9 qui est ici notre requ\u00eate Q. Voil\u00e0. Et donc, dans l'approche par paire, on va chercher \u00e0 d\u00e9terminer non plus le score de pertinence, mais quel document est plus pertinent qu'un autre. On va essayer de d\u00e9terminer la pr\u00e9f\u00e9rence d'un document par rapport \u00e0 un autre. Regardons de mani\u00e8re un peu plus d\u00e9taill\u00e9e comment on va pouvoir mettre ce principe en \u0153uvre. L'\u00e9l\u00e9ment fondamental, \u00e7a reste toujours ce couple, ou cette paire document-requ\u00eate qu'on appelle XI, et donc on consid\u00e8re qu'on a N couples, N \u00e9chantillons de paire document-requ\u00eate. \u00c7a, c'est notre ensemble d'apprentissage. Et ce qu'on souhaite trouver, c'est une fonction d'ordonnancement, c'est-\u00e0-dire qu'on souhaite trouver une fonction f qui va respecter l'ordre des X. Donc si XJ est pr\u00e9f\u00e9r\u00e9 \u00e0 XI, alors f de XJ doit \u00eatre sup\u00e9rieur \u00e0 f de XI. Donc, pour simplifier le d\u00e9veloppement, je vais consid\u00e9rer ici f qui est une fonction de classement lin\u00e9aire, et donc que je vais pouvoir \u00e9crire comme \u00e7a. \u00c7a, c'est le produit Scalar. En pratique, f, \u00e7a peut \u00eatre tout type de fonction, mais vraiment pour plus facilement vous expliquer ce d\u00e9veloppement-l\u00e0, on va consid\u00e9rer que f, c'est une fonction de classement lin\u00e9aire. Pour f lin\u00e9aire, j'ai donc, si f de XI est sup\u00e9rieur \u00e0 f de XJ, alors j'ai bien cette relation d'\u00e9quivalence. Donc ma fonction de classement lin\u00e9aire, elle est positive dans ce cas-l\u00e0 pour la diff\u00e9rence XI moins XJ. Donc pour f lin\u00e9aire, j'ai bien cette relation d'\u00e9quivalence. Je peux donc transformer mon probl\u00e8me de classement en un probl\u00e8me de classification binaire sur la diff\u00e9rence des XI moins XJ. Et c'est comme \u00e7a, en fait, que va fonctionner l'approche par paire. Et \u00e7a, c'est ce qu'on appelle la transformation par paire. C'est-\u00e0-dire qu'on va former, \u00e0 partir de nos donn\u00e9es, des exemples d'apprentissage sous la forme de paires \u00e9tiquet\u00e9es en utilisant la diff\u00e9rence entre deux couples, donc XI moins XJ. Et les \u00e9tiquettes que je vais donner, typiquement, elles vont \u00eatre binaires et je vais donner \u00e0 une diff\u00e9rence XI moins XJ la valeur de 1 si XI est pr\u00e9f\u00e9r\u00e9 \u00e0 XJ pour la requ\u00eate Q. Et puis sinon, je lui donne la valeur moins 1. Voil\u00e0, donc j'ai bien ces relations d'\u00e9quivalence, c'est-\u00e0-dire que si XI est pr\u00e9f\u00e9r\u00e9 \u00e0 XJ, alors YIJ est \u00e9gal \u00e0 plus 1. Et donc, j'ai bien ramen\u00e9 comme \u00e7a mon probl\u00e8me de classement en un probl\u00e8me de classification binaire. Donc, on va rapidement illustrer le principe de cette transformation par paire sur un petit exemple. Donc, on consid\u00e8re ici qu'on a deux requ\u00eates. Donc \u00e7a, c'est les donn\u00e9es qui correspondent \u00e0 la requ\u00eate 1 et \u00e7a, c'est les donn\u00e9es qui correspondent \u00e0 la requ\u00eate 2. On a donc des donn\u00e9es qui sont class\u00e9es et qui sont class\u00e9es selon une fonction de classement qu'on voit \u00eatre lin\u00e9aire. Voil\u00e0, typiquement, si on projette les donn\u00e9es sur cette droite lin\u00e9aire, sur cet hyperplan lin\u00e9aire, on voit qu'on peut effectivement l'utiliser comme une fonction de classement. A partir de ces donn\u00e9es, je vais donc appliquer ma transform\u00e9e par paire en prenant donc l'ensemble des donn\u00e9es associ\u00e9es \u00e0 un contexte donn\u00e9, donc \u00e0 une requ\u00eate donn\u00e9e, et qui ont des rangs diff\u00e9rents et en effectuant les diff\u00e9rences par paire. Et donc l\u00e0, typiquement, si je prends la paire X1, X2, alors je vais labelliser X1, X2 en plus 1 parce que X1 est pr\u00e9f\u00e9r\u00e9 \u00e0 X2 pour cette requ\u00eate avec cette fonction de classement. Et de m\u00eame, X2 va \u00eatre pr\u00e9f\u00e9r\u00e9 \u00e0 X3 pour cette requ\u00eate avec cette fonction de classement et donc je vais \u00e9tiqueter X2 moins X3 \u00e0 plus 1. Si maintenant je prends la paire X3, X1, je lui donnerai le label moins 1 parce que X3 n'est pas pr\u00e9f\u00e9r\u00e9 \u00e0 X1 pour cette requ\u00eate avec cette fonction de classement. Et donc en appliquant cette fonction de transformation par paire \u00e0 l'ensemble des donn\u00e9es qui appartiennent \u00e0 des rangs diff\u00e9rents et \u00e0 un m\u00eame contexte, alors je transforme effectivement mon probl\u00e8me en un probl\u00e8me de classification binaire. Donc on voit ici, pour les m\u00eames donn\u00e9es, le r\u00e9sultat de l'application de cette transformation. Donc on a bien ici nos instances positives et on a bien ici nos instances n\u00e9gatives et on voit sur cet exemple qu'on doit tr\u00e8s certainement pouvoir trouver une solution \u00e0 ce probl\u00e8me de classification en utilisant une technique de classification et notamment par exemple en utilisant des s\u00e9parateurs lin\u00e9aires. Voil\u00e0, donc juste des petites remarques, dans la constitution du jeu de donn\u00e9es, on ne consid\u00e8re que les donn\u00e9es d'un m\u00eame groupe qui sont prises en compte pour les paires de diff\u00e9rence et bien s\u00fbr, seules les donn\u00e9es de rang diff\u00e9rent dans ce m\u00eame groupe sont prises en compte. Je ne fais pas, je ne construis pas de paire de diff\u00e9rence en prenant des donn\u00e9es qui sont de groupe diff\u00e9rent, \u00e7a n'aurait pas de sens, c'est des donn\u00e9es qui sont non comparables. Voil\u00e0, donc je vais juste terminer pour cette pr\u00e9sentation de l'approche par paire en pr\u00e9sentant une approche qui s'appelle Ranking SVM et qui se base sur les s\u00e9parateurs \u00e0 vaste marge pour proposer une solution \u00e0 ce probl\u00e8me d'ordonnancement \u00e0 l'aide de cette transform\u00e9e par paire. Rapidement avant cela, je vais vous pr\u00e9senter ce que sont les s\u00e9parateurs \u00e0 vaste marge lin\u00e9aires. J'avais rapidement commenc\u00e9 \u00e0 pr\u00e9senter les SVM dans le cours pr\u00e9c\u00e9dent, notamment en pr\u00e9sentant ces m\u00e9thodes comme des m\u00e9thodes de classification dites discriminantes. Donc on a des donn\u00e9es d'apprentissage labellis\u00e9es, donc X, Y et on va se placer dans un cadre de classification binaire, donc avec des labels qui peuvent prendre les valeurs moins 1 et plus 1. Et je cherche \u00e0 construire \u00e0 partir de cet ensemble d'apprentissage, donc une fonction de d\u00e9cision qui va donc de l'espace de repr\u00e9sentation de mes donn\u00e9es \u00e0 moins 1 plus 1 et qui va permettre de pr\u00e9dire la classe moins 1 ou plus 1 d'un point X qui appartient \u00e0 X, n'importe quel point X qui appartient \u00e0 X. Voil\u00e0, ma fonction de d\u00e9cision, elle va de Rd dans R, Rd \u00e9tant l'espace de description de mes donn\u00e9es d'entr\u00e9e. Et puis je peux utiliser le signe de cette fonction pour classer mes donn\u00e9es. Donc typiquement, je vais affecter X \u00e0 la classe moins 1 si f2x est inf\u00e9rieur \u00e0 0 et \u00e0 la classe plus 1 si le signe de ma fonction de d\u00e9cision est sup\u00e9rieur \u00e0 0, appliqu\u00e9 \u00e0 X bien s\u00fbr. Si on fait un bref rappel \u00e0 la mani\u00e8re dont je vous ai pr\u00e9sent\u00e9 la th\u00e9orie de l'apprentissage supervis\u00e9, j'avais \u00e9voqu\u00e9 le fait qu'on avait une classe d'hypoth\u00e8ses pour notre fonction f. Ici, cette classe d'hypoth\u00e8ses, c'est qu'on a des fonctions de d\u00e9cision qui sont lin\u00e9aires et donc qui vont prendre cette forme analytique. Et ce que je vais chercher \u00e0 d\u00e9terminer, ce sont les param\u00e8tres de cette fonction, c'est-\u00e0-dire le vecteur W et B. Voil\u00e0, prenons un exemple simple, pla\u00e7ons-nous dans R2 et prenons donc un probl\u00e8me de classification lin\u00e9aire. Donc on a effectivement deux classes qui sont caract\u00e9ris\u00e9es ici par les croix rouges et les ronds bleus et je cherche effectivement une fonction de s\u00e9paration dans cet espace R2 qui soit lin\u00e9aire. On voit ici que les donn\u00e9es, que le plan, je peux le s\u00e9parer en deux par un hyperplan dont l'\u00e9quation est WTX plus B. Si je prends le signe de cet hyperplan, \u00e7a me permet d'avoir une fonction qui va me permettre de classer mes diff\u00e9rents points selon le signe que prend cette fonction quand je l'applique \u00e0 mes donn\u00e9es X. Ce qu'il est int\u00e9ressant de voir, c'est que la distance d'un point \u00e0 cet hyperplan, on peut l'exprimer avec cette \u00e9quation. Notamment, on verra l'importance de cette notion-l\u00e0 un peu plus tard. Et puis la distance de l'hyperplan \u00e0 l'origine, c'est la valeur absolue de B sur la norme de W. Si je reprends mon probl\u00e8me de s\u00e9paration binaire dans un espace \u00e0 deux dimensions, on voit qu'en fait j'ai plusieurs hyperplans qui sont possibles, plusieurs s\u00e9parateurs qui peuvent \u00eatre possibles. C'est-\u00e0-dire que cette droite-l\u00e0 va effectivement me permettre de s\u00e9parer correctement mes donn\u00e9es, celle-ci aussi et celle-ci aussi. Donc la question qu'il faut se poser ici, c'est quel est le s\u00e9parateur qui va \u00eatre optimal ? Le s\u00e9parateur optimal, l'hyperplan optimal, \u00e7a va \u00eatre celui qui va classifier correctement les donn\u00e9es et qui se trouve le plus loin possible de tous les exemples. Et \u00e7a, c'est \u00e0 des fins de g\u00e9n\u00e9ralisation. Cet hyperplan optimal, c'est l'hyperplan de marge maximale. La marge \u00e9tant la distance minimale entre un exemple et l'hyperplan, et donc la surface de s\u00e9paration. C'est donc cet hyperplan optimal que je vais chercher \u00e0 d\u00e9terminer. Voil\u00e0, juste quelques d\u00e9finitions. On parle de s\u00e9parateur lin\u00e9aire, donc il est n\u00e9cessaire de d\u00e9finir ce que sont les donn\u00e9es qui sont s\u00e9parables lin\u00e9airement. Et typiquement, on va dire que les couples XYI vont \u00eatre lin\u00e9airement s\u00e9parables s'il existe un hyperplan qui permet de discriminer correctement cet ensemble de donn\u00e9es. Dans le cas contraire, on va parler d'exemples qui ne sont pas s\u00e9parables. Pour limiter l'espace des possibilit\u00e9s, on va consid\u00e9rer que les points les plus proches de l'hyperplan s\u00e9parateur sont situ\u00e9s sur des hyperplans qu'on va appeler canoniques et qui sont d\u00e9finis par cette \u00e9quation. L'\u00e9quation WTX plus B est \u00e9gale \u00e0 1 et WTX plus B est \u00e9gale \u00e0 moins 1. Dans ce cas, la marge, on peut la d\u00e9finir comme \u00e9tant 2 sur la norme de W. Et d'autre part, les points vont \u00eatre bien class\u00e9s si, quels que soient I, donc pour tous les points de mon ensemble d'apprentissage, j'ai Y fois F de XI qui est sup\u00e9rieur \u00e0 1. \u00c7a, c'est des contraintes, des conditions qui m'expriment que chaque point est bien class\u00e9 par ma fonction F. Et en utilisant cette simplification, je peux cette fois-ci d\u00e9finir de mani\u00e8re beaucoup plus formelle ce que c'est que mon s\u00e9parateur \u00e0 vaste marge. Donc, \u00e9tant donn\u00e9 un ensemble de points lin\u00e9airement s\u00e9parables et tiquet\u00e9s, donc mon ensemble d'apprentissage qui contient N \u00e9chantillons, l'objectif va \u00eatre de trouver un hyperplan qui maximise la marge et qui discrimine correctement les points de D. Donc, la formulation math\u00e9matique du SVM, c'est celle-ci. Je cherche \u00e0 minimiser selon WB 1 demi de la norme de W\u00b2. Donc \u00e7a, c'est typiquement ce qui exprime le fait que je cherche \u00e0 maximiser ma marge sous contrainte que l'ensemble de mes points de mon ensemble d'apprentissage soit bien class\u00e9. C'est ce qu'on exprime avec cette \u00e9quation-l\u00e0. \u00c7a, c'est la formulation classique des s\u00e9parateurs \u00e0 vaste marge. Voil\u00e0, on a donc ici un probl\u00e8me de minimisation sous contrainte qu'on peut r\u00e9soudre par des approches num\u00e9riques comme de la programmation quadratique, par exemple en minimisant le carr\u00e9 de la norme. Je ne vais pas rentrer dans les d\u00e9tails de r\u00e9solution de ce type de probl\u00e8me d'optimisation. Ce que je peux vous dire, c'est qu'en fait pour les SVM, on va faire un passage au lacrang\u00e9, c'est-\u00e0-dire qu'on va utiliser la forme duale de ce probl\u00e8me d'optimisation pr\u00e9c\u00e9dent. On sait qu'un probl\u00e8me d'optimisation, il poss\u00e8de une forme duale si la fonction objectif et les contraintes sont strictement convexes et alors dans ce cas l\u00e0 la solution du probl\u00e8me dual est la solution du probl\u00e8me original. Donc on utilise cette propri\u00e9t\u00e9 l\u00e0 pour la r\u00e9solution des s\u00e9parateurs \u00e0 vaste marge. Et notamment on va donc passer au lagrangien, en introduisant dans le probl\u00e8me d'optimisation des multiplicateurs de Lagrange \u03b1i qui sont donc associ\u00e9s aux diff\u00e9rentes contraintes d'in\u00e9galit\u00e9 qui expriment que l'ensemble des points de mon ensemble d'apprentissage doivent \u00eatre bien class\u00e9s. Donc du coup j'introduis dans mon probl\u00e8me, dans la formulation de mon probl\u00e8me, n param\u00e8tres \u03b1i. Et donc la formulation de mon probl\u00e8me en passant au lagrangien \u00e7a devient celle-ci. Donc j'ai cette fonction objectif qui devient celle-ci avec donc les param\u00e8tres, le param\u00e8tre W, B, \u03b1 et donc cette fonction objectif ici qui est donc je cherche \u00e0 maximiser ma marge avec donc les contraintes que l'ensemble des donn\u00e9es de mon ensemble d'apprentissage doivent \u00eatre bien class\u00e9es. J'exprime exactement la m\u00eame chose. Donc j'ai une nouvelle formulation du probl\u00e8me en fait o\u00f9 la contrainte elle est directement int\u00e9gr\u00e9e dans la fonction objectif et donc \u00e7a devient int\u00e9ressant. Voil\u00e0, pour la r\u00e9solution des SVM il est important d'introduire des vecteurs particuliers qui sont ce qu'on appelle des vecteurs supports. Donc W1 je peux le d\u00e9finir comme \u00e9tant la somme sur l'ensemble de mes donn\u00e9es d'apprentissage de \u03b1i, yi, xi. Je sais que \u03b1i est nulle si yi fois Wtxi plus B est sup\u00e9rieur \u00e0 1. Donc W il n'est d\u00e9fini que par les points tels que j'ai cette \u00e9quation l\u00e0 qui est \u00e9gale \u00e0 1. Et ces points l\u00e0 c'est mes vecteurs supports, ce sont ces points l\u00e0. Ces points qui finalement appartiennent au plan canonique qu'on a introduit plus en avant. Voil\u00e0, donc en pratique les SVM on calcule W, on cherche finalement les valeurs, on estime W en utilisant les donn\u00e9es d'apprentissage pour r\u00e9soudre le dual, donc le Lagrangien. Et en faisant \u00e7a on obtient les param\u00e8tres \u03b1i. A partir des \u03b1i j'en d\u00e9duis donc l'estimation de mon vecteur W avec cette \u00e9quation l\u00e0. La somme pour l'ensemble de mes donn\u00e9es d'apprentissage de \u03b1i, xi, yi, xi. Puis je peux du coup calculer B en prenant en compte le fait que les \u03b1i qui sont sup\u00e9rieurs \u00e0 0 correspondent aux points supports qui v\u00e9rifient donc cette relation. Et donc je peux en d\u00e9duire comme \u00e7a la valeur de B. En pratique je fais la moyenne de ces termes pour l'ensemble des vecteurs supports SV pour obtenir une valeur num\u00e9rique qui est stable. Et donc j'obtiens ma fonction de d\u00e9cision que je peux exprimer comme \u00e7a, qui est donc la somme pour l'ensemble des vecteurs supports de \u03b1i, xi, xi, t, x. J'ai ici un produit scalaire en fait, je pourrais aussi l'exprimer avec l'autre notation des produits scalaires, plus B. Donc tout ce qu'on a vu c'est assez joli, mais \u00e7a ne marche que si on a des donn\u00e9es qui sont s\u00e9parables lin\u00e9airement. Or dans la vraie vie les donn\u00e9es elles sont rarement s\u00e9parables lin\u00e9airement. Il n'y a pas de raison qui fait que les donn\u00e9es sont s\u00e9parables lin\u00e9airement pour \u00e9norm\u00e9ment de probl\u00e8mes. Et donc on va voir comment on peut \u00e9tendre les s\u00e9parateurs \u00e0 basse de marge au cas non s\u00e9parable. Donc typiquement comment on va pouvoir modifier la formulation quand justement l'hypoth\u00e8se de s\u00e9parabilit\u00e9 lin\u00e9aire des points n'est pas v\u00e9rifi\u00e9e ? Voil\u00e0, typiquement dans ce genre de cas l\u00e0, les points blancs qui sont ici et les carr\u00e9s verts qui sont ici font qu'on n'a pas des donn\u00e9es qui sont s\u00e9parables lin\u00e9airement. Dans ce cas l\u00e0, on va rel\u00e2cher les contraintes qui expriment le fait que les points sont bien class\u00e9s. Donc ces contraintes l\u00e0, et on va rajouter dans ces contraintes des variables de rel\u00e2chement \u03b5i. Et puis bien s\u00fbr on va p\u00e9naliser ces rel\u00e2chements dans la fonction objectif. Et donc on obtient une nouvelle formulation des SVM dans le cas non s\u00e9parable qui est celle-ci. Donc on cherche toujours \u00e0 maximiser notre marge, \u00e0 trouver les valeurs des param\u00e8tres W et B qui maximisent notre marge, sous contrainte rel\u00e2ch\u00e9e que les points sont bien class\u00e9s. Donc l\u00e0 on introduit nos variables de rel\u00e2chement qu'on p\u00e9nalise dans la fonction objectif, ici. Et donc ce C l\u00e0, c'est une contrainte de r\u00e9gularisation justement li\u00e9e \u00e0 cette p\u00e9nalisation des variables de rel\u00e2chement, qui est un hyperparam\u00e8tre des SVM. Et puis on a encore un cas qui est un peu plus compliqu\u00e9, qui est le cas o\u00f9 la s\u00e9paration n'est pas lin\u00e9aire. Et donc on voit bien qu'ici la droite qui s\u00e9pare, la fonction qui s\u00e9pare nos points, qui sont s\u00e9parables, mais cette fonction l\u00e0 n'est clairement pas lin\u00e9aire. Comment on peut traiter ce cas ? Donc on va prendre un exemple aussi jou\u00e9, un exemple synth\u00e9tique qui est celui-l\u00e0. On voit qu'effectivement on a des donn\u00e9es qui sont dans un espace en deux dimensions, qui ne sont pas s\u00e9parables lin\u00e9airement. Et pour traiter ce probl\u00e8me-l\u00e0, avec les SVM, on va utiliser une astuce qu'on appelle l'astuce des noyaux. Typiquement, on va \u00e9tendre notre probl\u00e8me \u00e0 un cas o\u00f9 les donn\u00e9es vont \u00eatre s\u00e9parables lin\u00e9airement. \u00c7a c'est une extension justement des SVM \u00e0 des s\u00e9parateurs non lin\u00e9aires. Et pour \u00e7a, on va transposer les donn\u00e9es dans un autre espace, dans lequel les donn\u00e9es vont \u00eatre cette fois-ci lin\u00e9airement s\u00e9parables. Donc on va appliquer \u00e0 nos donn\u00e9es une transformation qui va de Rd dans H, un espace de Hilbert. Cette transformation, c'est le phi ici, qui est telle qu'on part d'un espace de d\u00e9part dans lequel les donn\u00e9es ne sont pas s\u00e9parables lin\u00e9airement, vers un espace d'arriv\u00e9e o\u00f9 elles le sont. Voil\u00e0, si on revient \u00e0 notre exemple, on a bien un espace initial \u00e0 deux dimensions dans lequel il n'est pas possible de s\u00e9parer lin\u00e9airement nos donn\u00e9es. Et donc je dois choisir une transformation phi qui doit permettre une s\u00e9paration lin\u00e9aire dans un nouvel espace H. Ici, je peux par exemple prendre cette transformation phi, qui d\u00e9pend donc de X1 et X2, et qui transforme mes donn\u00e9es dans un espace \u00e0 trois dimensions, avec X1, X2 et X1\u00b2 plus X2\u00b2. Si je trace mes donn\u00e9es dans ce nouvel espace, qui est un espace \u00e0 trois dimensions, qui est un espace de plus grande dimension que l'espace original, on voit qu'ici j'ai effectivement une possibilit\u00e9 de s\u00e9parer mes donn\u00e9es par un s\u00e9parateur lin\u00e9aire. On voit qu'on a effectivement un hyper-s\u00e9parateur. C'est vraiment cette id\u00e9e-l\u00e0 qu'on veut mettre en place pour utiliser les s\u00e9parateurs \u00e0 vaste marge quand on a des cas de s\u00e9paration non lin\u00e9aire. Ce qui est compliqu\u00e9, c'est finalement le choix de cette transformation, le changement de repr\u00e9sentation, comment je peux faire. On souhaite avoir un changement de repr\u00e9sentation qui va permettre une s\u00e9paration lin\u00e9aire entre nos deux classes, tout en respectant bien s\u00fbr la vraie similarit\u00e9 entre les donn\u00e9es. En g\u00e9n\u00e9ral, \u00e7a veut dire que je dois trouver ce qu'on appelle un espace de redescription de plus grande dimension. Les questions, c'est comment je peux faire \u00e7a ? Je peux faire \u00e7a par exemple par un s\u00e9parateur lin\u00e9aire, je dois trouver ce qu'on appelle un espace de redescription de plus grande dimension. Les questions, c'est comment je peux trouver cet espace de redescription et comment je peux garantir la r\u00e9alisation des calculs ? Pour \u00e7a, c'est l\u00e0 qu'il y a vraiment l'astuce des fonctions noyaux, qui fait qu'on va \u00e9viter de calculer explicitement la transformation \u03a6, et on va plut\u00f4t s'appuyer sur des fonctions particuli\u00e8res qui sont des fonctions noyaux. Plut\u00f4t qu'appliquer la transformation de changement de repr\u00e9sentation, on va d\u00e9finir une fonction noyau K, telle que K de XI, XJ, c'est \u00e9gal au produit scalaire de la transform\u00e9e de XI et de la transform\u00e9e de XJ. Ce produit scalaire dans l'espace de redescription. Je cherche une fonction K qui correspond au produit scalaire dans mon espace H, qui est mon espace de redescription. Il y a des travaux th\u00e9oriques qui prouvent que de telles fonctions existent. C'est notamment le th\u00e9or\u00e8me de Mercer qui dit qu'une fonction noyau K continue sym\u00e9trique et semi-d\u00e9finie positive peut s'exprimer comme un produit scalaire dans un espace de plus grande dimension. En utilisant cette fonction noyau, la fonction de d\u00e9cision dans l'espace d'origine devient celle-ci. La somme pour Y qui appartient au vecteur support de \u03b1I, YI, la fonction noyau appliqu\u00e9e \u00e0 XI, X, plus P. On appelle la fonction noyau toute fonction K, qui va de mon espace originelle dans R, et qui peut \u00eatre interpr\u00e9t\u00e9e comme un produit scalaire dans un plongement \u03a6. Le gros avantage, c'est qu'on peut maintenant appliquer tous les algorithmes qu'on a vus pr\u00e9c\u00e9demment de s\u00e9paration optimale avec marge souple ou dure, donc avec les contraintes de rel\u00e2chement, en rempla\u00e7ant le produit scalaire entre XI et XJ par la fonction noyau K de XI, XJ. J'obtiens ainsi un classifieur exprim\u00e9 comme \u00e7a, et qui est lui lin\u00e9aire dans l'espace de plongement. Il existe \u00e9norm\u00e9ment de travaux sur des fonctions noyaux, il existe \u00e9norm\u00e9ment de fonctions noyaux, par exemple les noyaux polynomials, le noyau gaussien, le noyau sigmo\u00efde, etc. Le choix du noyau est tr\u00e8s important dans la mise en \u0153uvre des SVM, parce qu'il doit maximiser les chances d'\u00eatre dans le bon espace. \u00c7a fait aussi partie des hyper param\u00e8tres des SVM, le choix du noyau. Enfin, si je veux en finir avec les SVM, je dois parler du cas o\u00f9 on a plusieurs classes. On n'a parl\u00e9 ici que des SVM dans le cadre binaire, c'est un probl\u00e8me de classification binaire. Comment transformer tout ce qu'on a vu pr\u00e9c\u00e9demment, ou comment l'adapter \u00e0 un cas o\u00f9 j'ai plusieurs classes ? Ici, je n'ai plus deux classes, mais j'ai ces classes CI, et donc typiquement, telle que c'est, et plus grande que 2. Il y a plusieurs strat\u00e9gies pour appliquer les SVM au cadre multiclasse. Il y a une strat\u00e9gie qui s'appelle le one versus all. \u00c7a consiste \u00e0 prendre pour une classe CI ses exemples positifs, et de ramener un probl\u00e8me de classification binaire en prenant tous les exemples des autres classes CI diff\u00e9rents de I qui valent moins 1. Je me ram\u00e8ne ainsi un probl\u00e8me de classification binaire. Je vais apprendre ces classifiers binaires. Pour faire mon classement, je vais retenir la classe qui a le plus fort score comme classe pour mes donn\u00e9es. Une autre strat\u00e9gie s'appelle le one versus one. Cette fois-ci, je vais prendre les exemples positifs d'une classe I1 et les exemples n\u00e9gatifs d'une classe I2. Je vais apprendre de cette mani\u00e8re ces fois-ci moins 1 sur 2 classifiers binaires. Il faut bien s\u00fbr que je fasse s'affronter mes diff\u00e9rents classifiers. Je vais mettre en place un processus de tournoi et je vais voter pour chaque classifier. Une classe va gagner \u00e0 chaque fois. En agr\u00e9geant mes votes, je vais obtenir le vainqueur et la classe que je vais pouvoir affecter \u00e0 mes donn\u00e9es. Un petit bilan sur les SVM. C'est une approche d'apprentissage qui est relativement puissante et qui est capable de trouver des motifs non lin\u00e9aires. C'est int\u00e9ressant avec deux id\u00e9es principales. Cette maximisation de la marge entre la fronti\u00e8re de d\u00e9cision et les exemples les plus proches, qui sont les vecteurs supports. Et puis l'astuce des noyaux avec une redescription des observations dans un nouvel espace o\u00f9 une s\u00e9paration lin\u00e9aire sera possible dans le cas o\u00f9 elle ne l'est pas dans l'espace originel. Et puis pour finir, une extension facile au cas multiclasse. Apr\u00e8s cette grosse parenth\u00e8se sur les SVM, je peux revenir \u00e0 mon probl\u00e8me d'ordonnancement avec l'approche de transformer par paire. Et donc vous pr\u00e9senter la m\u00e9thode ranking SVM. Typiquement, je peux directement utiliser cette m\u00e9thode des SVM pour trouver un hyperplan s\u00e9parateur de mes donn\u00e9es g\u00e9n\u00e9r\u00e9es en appliquant la transform\u00e9e par paire. Mes donn\u00e9es d'apprentissage, c'est ppaire x1, x2, labellis\u00e9es comme pr\u00e9sent\u00e9es pr\u00e9c\u00e9demment. La formulation de ce probl\u00e8me en utilisant les SVM devient celle-l\u00e0. Je cherche \u00e0 maximiser ma marge. Ici, c'est la formulation avec des marges souples, avec mes contraintes de rel\u00e2chement. Sous contrainte que mes ppaires d'apprentissage, exprim\u00e9es comme la diff\u00e9rence de mes paires originelles, soient bien class\u00e9es, avec mes contraintes de rel\u00e2chement, etc. Je sais r\u00e9soudre ce type de probl\u00e8me avec des techniques d'optimisation. La solution du probl\u00e8me me donne une solution optimale, qui est finalement l'estimation de W. Comment je peux classer ? Je sais que x' est pr\u00e9f\u00e9r\u00e9 \u00e0 x si et seulement si le signe du classifiaire est n\u00e9gatif. Si le signe de W \u00e9toile x-x' est n\u00e9gatif. Je pourrais classer en utilisant cette \u00e9quation et ce m\u00e9canisme. On va directement utiliser le score du mod\u00e8le qu'on vient d'apprendre. Cela r\u00e9pond aussi \u00e0 la deuxi\u00e8me question, qui est comment trier les documents pour de nouvelles requ\u00eates. Pour chaque requ\u00eate, je vais consid\u00e9rer mon x' qui vaut le vecteur nul, et je vais trier le document sur le score obtenu directement par la fonction W \u00e9toile x-x'. Ce score obtenu est celui-l\u00e0, avec x qui est toujours une paire document requ\u00eate. Cela ne change pas, c'est toujours l'\u00e9l\u00e9ment principal de ces approches Learning to Rank. Il y a bien s\u00fbr plein d'autres approches pour faire des strat\u00e9gies Learning to Rank \u00e0 partir de l'approche par paire, que je ne pr\u00e9senterai pas ici, mais juste pour r\u00e9sumer l'approche par paire. On a des donn\u00e9es d'entr\u00e9e qui sont des donn\u00e9es ordonn\u00e9es, mais de paire de vecteurs. On consid\u00e8re les documents par paire et pas les documents ind\u00e9pendamment comme dans l'approche par point. Je me ram\u00e8ne \u00e0 un probl\u00e8me de classification binaire avec l'approche de transformer par paire. J'apprends ce classifieur qui, en utilisant son score, va me permettre de faire le classement. J'ai ici une fonction de co\u00fbt paireoise de classification. Pour le classement, j'ai juste \u00e0 trier les valeurs de sortie de mon classifieur. Je peux donc proposer \u00e0 l'utilisateur une liste ordonn\u00e9e de documents, que j'\u00e9value avec les mesures d'\u00e9valuation classique d'ordonnancement. La derni\u00e8re approche possible pour l'apprentissage par ordonnancement, c'est l'approche par liste. Le principe est simple, c'est de traiter directement les listes tri\u00e9es comme des exemples d'apprentissage. Directement d'apprendre \u00e0 partir de listes tri\u00e9es. Pour \u00e7a, il y a deux types d'approches. Il y a des approches qui ont dans leur fonction objectif des \u00e9l\u00e9ments li\u00e9s aux mesures d'\u00e9valuation. Par exemple, il y a une approche qui s'appelle SVM Map, qui utilise des vecteurs SVM. La fonction objectif est la borne sup\u00e9rieure de la mesure d'\u00e9valuation Map, le mean average precision, dont on a parl\u00e9 pendant le cours d'\u00e9valuation. Il y a aussi des fonctions objectives qui sont directement d\u00e9finies sur des listes de documents. C'est compliqu\u00e9 ces mesures-l\u00e0, parce qu'en g\u00e9n\u00e9ral, on est sur des mesures d'\u00e9valuation non continues. On rentre dans des probl\u00e9matiques de diff\u00e9renciabilit\u00e9 et \u00e7a devient un peu compliqu\u00e9. Je ne pr\u00e9senterai pas d'approche tr\u00e8s d\u00e9taill\u00e9e sur cette approche par liste, parce que chaque approche n\u00e9cessiterait de gros d\u00e9veloppements. Le petit bilan sur les approches par liste, avec un m\u00eame m\u00e9canisme pour l'apprentissage et le classement. On fait une permutation sur cette fonction F, et on a donc une fonction de co\u00fbt qui est la m\u00eame pour l'apprentissage et le classement. Parce que typiquement, on s'appuie sur les mesures d'\u00e9valuation de classement pour construire cette fonction de co\u00fbt. On a vu que quand on est dans des approches \u00e0 base d'apprentissage, il est n\u00e9cessaire d'avoir des donn\u00e9es d'apprentissage. La question c'est, quelles sont les donn\u00e9es d'apprentissage \u00e0 disposition pour mettre en \u0153uvre ce type d'approche ? Il existe un certain nombre de donn\u00e9es de benchmark qui sont disponibles. Il y a des donn\u00e9es qui sont annot\u00e9es pour plusieurs collections, notamment les collections TREC, qui sont les challenges classiques en \u00e9valuation dans la recherche d'informations, cl\u00e9 pour tout ce qui est multilinguisme. NTCIR, c'est plut\u00f4t des donn\u00e9es m\u00e9dicales. Les TOR, qui sont vraiment des donn\u00e9es qui ont \u00e9t\u00e9 mises \u00e0 disposition pour les paradigmes Learning to Rank. Et puis, il y a Yahoo qui a aussi mis \u00e0 disposition beaucoup de donn\u00e9es. Pour les entreprises, c'est un peu plus compliqu\u00e9 parce qu'on n'a pas de telles donn\u00e9es en g\u00e9n\u00e9ral, et il faut pouvoir les constituer. Dans le cas du web, c'est un petit peu diff\u00e9rent parce qu'on a une information tr\u00e8s riche qui provient du clic des utilisateurs. Et \u00e0 partir de cette information de clic, on peut inf\u00e9rer des pr\u00e9f\u00e9rences entre documents, et donc on peut inf\u00e9rer tr\u00e8s facilement des paires de pr\u00e9f\u00e9rences. C'est pour \u00e7a que l'approche par paire est tr\u00e8s d\u00e9velopp\u00e9e dans les approches d'apprentissage pour l'ARI, et notamment dans le contexte du web. Alors, il faut faire attention bien s\u00fbr parce que les clics, ils ne fournissent pas de jugement de pertinence absolue mais relatif, et donc c'est ce qu'on veut pour l'approche par paire. Et donc typiquement, il y a \u00e9norm\u00e9ment de strat\u00e9gies qui peuvent \u00eatre mises en place pour constituer ces donn\u00e9es d'apprentissage. Donc consid\u00e9rons un ordre D1, Dn par exemple, et c'est un ensemble de documents cliqu\u00e9s. Alors, on pourrait par exemple utiliser cette strat\u00e9gie pour construire un ordre de pertinence entre documents. Donc si un document Di appartient \u00e0 C et que Di n'appartient pas \u00e0 C, forc\u00e9ment Di est pr\u00e9f\u00e9r\u00e9 pour la requ\u00eate Q \u00e0 Di. Voil\u00e0, si Di c'est le dernier document cliqu\u00e9, et quel que soit J qui est inf\u00e9rieur \u00e0 I, alors Di n'appartient pas \u00e0 C, alors Di est pr\u00e9f\u00e9r\u00e9 \u00e0 Dj pour la requ\u00eate Q. Et puis, etc etc. On peut mettre en place \u00e9norm\u00e9ment de strat\u00e9gies comme \u00e7a pour construire des ordres de pertinence entre documents. Voil\u00e0, alors il faut \u00eatre quand m\u00eame vigilant avec \u00e7a, et avec l'exploitation des clics comme substitut de pertinence, parce que c'est fortement biais\u00e9. Voil\u00e0, typiquement prenons un exemple, avec la requ\u00eate \u00e0 Implicit Feedback in Information Retrieval, l'utilisateur clique sur le premier document. Est-ce que pour autant on peut dire que le premier document est plus pertinent que le deuxi\u00e8me pour ce besoin d'information ? C'est tr\u00e8s discutable. Voil\u00e0, de m\u00eame, un utilisateur ne clique pas sur un des premiers documents. Est-ce qu'on peut pour autant dire que le deuxi\u00e8me est beaucoup plus pertinent que le premier ? C'est pareil, c'est tr\u00e8s discutable. Et c'est des choix d'interpr\u00e9tation des clics, qui sont des choix de conception. Le clic, \u00e7a peut \u00eatre aussi un \u00e9l\u00e9ment de pertinence tr\u00e8s brut\u00e9. Par exemple, ici, j'ai toujours la m\u00eame requ\u00eate, Implicit Feedback in Information Retrieval, mais parce que mon moteur de recherche n'est pas tr\u00e8s bon, il me renvoie cette liste d'images, je me demande pourquoi, \u00e7a m'interroge, \u00e7a attire ma curiosit\u00e9, je clique. Prenons aussi un autre cas qui est difficile \u00e0 interpr\u00e9ter, o\u00f9 cette fois-ci, ma requ\u00eate, c'est la capitale de Londres. Donc, je vais me demander pourquoi, je vais me demander si je peux me faire un lien avec ce document, je vais me demander si je peux me faire un lien avec ce document, je vais me demander si je peux me faire un lien avec ce document, je vais me demander si je peux me faire un lien avec ce document, o\u00f9 cette fois-ci, ma requ\u00eate, c'est la capitale de Londres. L'utilisateur, fois ces r\u00e9sultats-l\u00e0, il quitte son aviateur. On pourrait interpr\u00e9ter \u00e7a comme, il n'est pas satisfait des r\u00e9sultats du moteur de recherche. Le probl\u00e8me, c'est qu'il va exactement avoir le m\u00eame comportement pour cette requ\u00eate-l\u00e0 et ces r\u00e9sultats-l\u00e0. Pourquoi ? Parce qu'il a directement la r\u00e9ponse \u00e0 sa question. Et donc, on voit bien toute la difficult\u00e9, finalement, qui est li\u00e9e \u00e0 l'analyse des clics et notamment \u00e0 l'analyse des clics comme substitut de cette notion de pertinence. C'est juste des petits exemples pour vous montrer \u00e0 quel point tout cela est forc\u00e9ment bruit\u00e9 et \u00e0 quel point \u00e7a peut introduire des biais et m\u00eame des erreurs dans l'exploitation des clics, justement pour l'am\u00e9lioration des moteurs de recherche. Voil\u00e0, juste pour en finir sur les jeux de donn\u00e9es, une rapide pr\u00e9sentation du jeu de donn\u00e9es Lettor, qui est un petit peu le jeu de donn\u00e9es qui est devenu tr\u00e8s standard pour l'\u00e9valuation des algorithmes d'ordonnancement. C'est un jeu de donn\u00e9es qui a \u00e9t\u00e9 propos\u00e9 par Microsoft Research avec un certain nombre de collections propos\u00e9es dans le jeu de donn\u00e9es. Et puis, il y a eu \u00e9norm\u00e9ment d'autres initiatives, notamment Yahoo qui a propos\u00e9 un challenge autour de l'apprentissage pour l'ordonnancement, et puis ensuite Microsoft, avec des jeux de donn\u00e9es \u00e0 chaque fois de plus en plus gros. Voil\u00e0, donc juste des ordres de grandeur, donc Lettor, on a une dizaine de jeux de donn\u00e9es avec \u00e0 chaque fois un certain nombre de requ\u00eates et pour chaque requ\u00eate, un certain nombre de documents associ\u00e9s, soit exprim\u00e9s sous paires, soit donn\u00e9s comme une liste de documents pertinents ordonn\u00e9s. Et puis aussi un certain nombre de variables descriptives des donn\u00e9es. Voil\u00e0, typiquement dans Lettor, on a chaque paire de donn\u00e9es requ\u00eates documents et d\u00e9finies selon un certain nombre de variables, dont ces 37 l\u00e0, donc qui sont typiquement les caract\u00e9ristiques qu'on utilise, que vous avez essay\u00e9 d'utiliser dans le Lab 1 pour caract\u00e9riser aussi vos donn\u00e9es d'apprentissage. Alors, il y a un probl\u00e8me dont on n'a pas vraiment trop parl\u00e9 jusqu'\u00e0 maintenant, qui est un probl\u00e8me de r\u00e9duction de la dimension et qui est nettement li\u00e9 au fait que l'ensemble de nos vecteurs caract\u00e9ristiques sont d\u00e9crits dans un espace de tr\u00e8s grande dimension, un espace qui est engendr\u00e9 par le vocabulaire de termes, donc avec autant de termes dans le vocabulaire en fait que de dimensions dans l'espace de repr\u00e9sentation. Et donc, il peut se poser un certain nombre de questions pour essayer de r\u00e9duire cet espace de repr\u00e9sentation. Et notamment, on peut faire \u00e7a en utilisant \u00e9norm\u00e9ment de techniques de s\u00e9lection de variables. Donc l'objectif, c'est de r\u00e9duire la grande dimensionnalit\u00e9 de l'espace de repr\u00e9sentation des documents. Avec plusieurs approches, il y a des approches qui consistent \u00e0 faire du seuil sur la mesure des f, o\u00f9 typiquement on va supprimer les termes qui ont une faible fr\u00e9quence dont on peut penser qu'ils ne sont pas informatifs pour une classe de donn\u00e9es. Donc \u00e7a, c'est des choses qu'on va beaucoup utiliser quand on est dans un cadre, par exemple, de cat\u00e9gorisation de documents. Et puis, il y a d'autres mesures qui se basent plut\u00f4t sur des mesures d'estimation de l'information, avec notamment la mesure d'information mutuelle ponctuelle, qui consiste \u00e0 estimer l'information que la pr\u00e9sence d'un terme apporte \u00e0 la classe C et apporte \u00e0 la cat\u00e9gorisation de cette classe. Et puis, l'information mutuelle qui s'int\u00e9resse \u00e0 la pr\u00e9sence et l'absence de termes et ce qu'ils apportent sur la cat\u00e9gorisation de la classe. Donc, je ne d\u00e9taillerai pas ces approches ici, mais vous avez une version d\u00e9taill\u00e9e des slides sur Edunao. Voil\u00e0, et j'aimerais juste terminer ce cours par une petite ouverture et justement, qui traite de cet aspect relatif \u00e0 la repr\u00e9sentation de l'information. Donc, on a utilis\u00e9 jusqu'\u00e0 maintenant des repr\u00e9sentations vectorielles et donc sur un espace qui est engendr\u00e9 par le vocabulaire de termes, avec comme dimension la cardinalit\u00e9 du vocabulaire de termes. Donc, voil\u00e0, on peut repr\u00e9senter le document de mani\u00e8re binaire en termes de pr\u00e9sence ou absence des termes de ce vocabulaire. On peut aussi repr\u00e9senter un terme de cette mani\u00e8re-l\u00e0, c'est-\u00e0-dire que le terme sur l'espace engendr\u00e9 par le vocabulaire de termes, il a une repr\u00e9sentation de l'espace. Il a une repr\u00e9sentation one-hot, donc il y a 1 pour la dimension qui correspond au terme et 0 pour les autres dimensions. Et puis, bien s\u00fbr, tout \u00e7a, on peut le repr\u00e9senter avec de la pond\u00e9ration, en avis \u00e0 quel point la pond\u00e9ration \u00e9tait importante en recherche d'informations. Voil\u00e0, alors il y a des grosses limitations \u00e0 ce type de repr\u00e9sentation et notamment, en fait, tr\u00e8s li\u00e9es \u00e0 la compr\u00e9hension, l'\u00e9tude fine de la langue et ce qu'on pourrait en faire en recherche d'informations. Et notamment, il est tr\u00e8s difficile de traiter des probl\u00e8mes de polys\u00e9mie et de synonymie avec ce type de repr\u00e9sentation. Ce type de repr\u00e9sentation, elles font toujours l'hypoth\u00e8se que les mots sont ind\u00e9pendants, alors on sait bien que les mots ne sont pas ind\u00e9pendants. Et puis, finalement, ce ne sont pas des repr\u00e9sentations qui arrivent \u00e0 capter r\u00e9ellement la s\u00e9mantique d'un mot. Voil\u00e0, et donc, on pourrait vouloir essayer d'am\u00e9liorer ces repr\u00e9sentations des mots et donc des documents en essayant de trouver des repr\u00e9sentations qui portent un peu plus la s\u00e9mantique des mots. Par exemple, pour faire de l'expansion de requ\u00eates en prenant en compte cette s\u00e9mantique. Supposons que vous cherchiez \u00e0 faire une requ\u00eate jaguar, est-ce qu'on parle de jaguar la voiture, est-ce qu'on parle de jaguar animal ? Voil\u00e0, c'est typiquement ce type de s\u00e9mantique qu'on aimerait capter et associer aux mots. Donc, l'expansion de requ\u00eates, c'est une technique de recherche d'informations qui a principalement comme objectif d'augmenter le rappel des syst\u00e8mes de recherche. On va typiquement \u00e9tendre la requ\u00eate en utilisant un certain nombre de ressources globales qui sont non-d\u00e9pendantes de la requ\u00eate. Alors, il y a trois approches principales, des techniques d'expansion de requ\u00eates avec des th\u00e9saurus existants, donc par exemple WordNet ou PuMed, une expansion avec une g\u00e9n\u00e9ration automatique de th\u00e9saurus, et puis il y a aussi, bien s\u00fbr, des expansions de requ\u00eates de l'ordre de la correction orthographique dont on ne parlera pas ici. Voil\u00e0, donc le principe, c'est que chaque terme de la requ\u00eate va pouvoir \u00eatre \u00e9tendu avec des synonymes ou des termes qui sont s\u00e9mantiquement proches en utilisant un th\u00e9saurus. Donc, on va voir justement comment on peut capter ces termes s\u00e9mantiquement proches avec donc diff\u00e9rentes approches. Donc, la notion de th\u00e9saurus, elle est multiple. Soit on a des th\u00e9saurus manuels qui sont maintenus par des th\u00e9saurus, soit on a des th\u00e9saurus manuels qui sont maintenus par des \u00e9diteurs, comme par exemple PuMed ou WordNet. Soit on peut d\u00e9river automatiquement des th\u00e9saurus \u00e0 partir notamment de statistiques de co-occurrence entre mots sur une collection. Donc, on prend en compte aussi le contexte des mots dans une collection. Et on peut aussi, bien s\u00fbr, utiliser l'analyse des logs des requ\u00eates et en faisant de la reformulation de requ\u00eates en utilisant les requ\u00eates qui ont \u00e9t\u00e9 formul\u00e9es par d'autres utilisateurs. Donc, si je pars d'un th\u00e9saurus existant, automatiquement, ce qui est fait dans le site PuMed, qui est un site qui permet d'acc\u00e9der \u00e0 des articles m\u00e9dicaux, ou voil\u00e0, si je fais une requ\u00eate cancer, automatiquement, ma requ\u00eate va \u00eatre \u00e9tendue avec les termes n\u00e9oplasme et cancer, qui sont en fait des mots d'une ressource terminologique qui s'appelle MeSH, et qui sont donc des termes qui sont beaucoup plus m\u00e9dicaux. Je peux aussi utiliser WordNet. Alors, WordNet, c'est quoi ? C'est une ressource lexicale, qui est certainement la ressource lexicale qui est la plus utilis\u00e9e en traitement du langage naturel. Elle est d\u00e9velopp\u00e9e \u00e0 Princeton depuis 1985. Et en gros, elle est constitu\u00e9e de ce qu'on appelle un ensemble de synsets, qui est en fait un groupe de sens ou des synonymes. Et ces synsets, ils sont reli\u00e9s entre eux par des relations lexicales et s\u00e9mantico-conceptuelles. Donc, c'est une vraie grosse ressource lexicale qui est int\u00e9ressante et qui a \u00e9t\u00e9 port\u00e9e en plusieurs langues. Et elle a aussi l'avantage d'\u00eatre disponible dans NLTK. Donc voil\u00e0, par exemple, prenons une requ\u00eate dog. Voil\u00e0, j'importe NLTK et j'importe WordNet. Et je demande finalement d'acc\u00e9der \u00e0 l'ensemble des synsets qui sont associ\u00e9s au terme dog. Voil\u00e0, on voit qu'il y en a un certain nombre de synsets qui sont associ\u00e9s au terme dog, dont tous ceux-l\u00e0. Et on voit qu'on a une d\u00e9finition associ\u00e9e. Et donc, du coup, je peux tout \u00e0 fait utiliser cette d\u00e9finition pour faire de l'expansion de requ\u00eate. Typiquement, je peux aussi prendre un ordre dans les synsets qui me sont renvoy\u00e9s, qui sont des ordres de fr\u00e9quence, dans ce m\u00e9canisme d'expansion de requ\u00eate. Je peux aussi vouloir cr\u00e9er mon propre th\u00e9saurus de co-occurrence en faisant de l'analyse de la distribution des mots dans un document. Typiquement, \u00e7a se base sur une similarit\u00e9 des mots qui est beaucoup plus contextuelle que r\u00e9ellement s\u00e9mantique. C'est-\u00e0-dire qu'on va prendre en compte une similarit\u00e9, on va consid\u00e9rer que deux mots sont similaires s'ils co-occurrent avec les m\u00eames mots. Typiquement, voiture et moto sont similaires parce qu'ils vont co-occurrer tr\u00e8s souvent dans un corpus avec les mots route et essence, qui n'est pas tout \u00e0 fait la m\u00eame chose que la similarit\u00e9 quand ils co-occurrent avec la m\u00eame relation grammaticale. Et donc, ce th\u00e9ausaurus de co-occurrence, en fait, on peut le construire avec des outils dont on a d\u00e9j\u00e0 parl\u00e9, notamment \u00e0 partir de la matrice d'incidence terme document, ou WIJ, c'est le poids normalis\u00e9 pour le terme TI dans le document DJI. Si je construis la matrice de co-occurrence C en faisant A fois la transform\u00e9e de A, eh bien, j'obtiens donc ma matrice de co-occurrence et pour chaque terme TI, je peux prendre les termes qui sont maximum dans C, bien s\u00fbr en utilisant ce que j'obtiens avec les points. Et voil\u00e0, par exemple, ce que je peux obtenir sur un exemple donn\u00e9, qui sont donc les termes qui co-occurrent avec un certain nombre de mots donn\u00e9s en utilisant cette approche. Et typiquement, je pourrais du coup faire de l'expansion de requ\u00eates en utilisant ces plus proches voisins dans mon th\u00e9aurus de co-occurrence. Par exemple, lithogra, je pourrais l'\u00e9tendre avec drawing, Picasso, Dali, etc. On voit que du coup, avec cette approche, la qualit\u00e9 des associations est souvent discutable. En plus de \u00e7a, on a souvent une matrice qui est fortement creuse, on avait d\u00e9j\u00e0 le probl\u00e8me avec la matrice terme incidence, et du coup, l'imbigu\u00eft\u00e9 des termes peut quand m\u00eame introduire de mauvaises associations entre nos termes. Et donc, on va essayer de trouver une nouvelle approche, et notamment, l'approche, elle va consister \u00e0 essayer d'apprendre directement les relations entre les termes. Donc, revenons \u00e0 notre probl\u00e8me de la relation entre termes. La repr\u00e9sentation des termes. Nos termes, ils sont consid\u00e9r\u00e9s comme des symboles atomiques. Chaque terme, c'est une dimension, et donc, on l'a vu avec une repr\u00e9sentation de type ouanote. Voil\u00e0, prenons par exemple le terme motel et le terme h\u00f4tel, et leur repr\u00e9sentation ouanote. Supposons que notre requ\u00eate contienne le terme motel, et que les documents qui sont dans notre corpus ne parlent que de termes h\u00f4tel. Et bien, typiquement, c'est l'inverse ici, mais si j'ai une requ\u00eate sur h\u00f4tel et que le document parle de motel, alors avec les principes de recherche qu'on a mis en \u0153uvre, qui font appel, du coup, au produit scalaire, je vais forc\u00e9ment avoir une similarit\u00e9 qui va \u00eatre nulle entre ma requ\u00eate et mon document. Donc, c'est vraiment ce probl\u00e8me-l\u00e0, en fait, qui est limitant et pour lequel on aimerait apporter une solution. Et typiquement, ce qu'on aimerait, c'est avoir une meilleure repr\u00e9sentation de nos termes. Donc, l'id\u00e9e, \u00e7a va \u00eatre d'apprendre une repr\u00e9sentation qui va \u00eatre de plus petite dimension d'un mot donn\u00e9, d'un terme, donc dans RD, et telle que cette repr\u00e9sentation U, elle soit telle que UTV repr\u00e9sente la similarit\u00e9 entre mots. Donc, le principe, \u00e7a va \u00eatre de repr\u00e9senter chaque mot par ce qu'on appelle un vecteur distributionnel. Et donc, en gros, on va repr\u00e9senter un mot \u00e0 l'aide de ses voisins. C'est pour \u00e7a qu'on appelle \u00e7a un vecteur distributionnel. Et donc, \u00e7a, \u00e7a utilise un principe qui est un principe qui date des ann\u00e9es 50, qui a \u00e9t\u00e9 \u00e9nonc\u00e9 par First, et qui dit qu'on peut conna\u00eetre la compagnie, qu'on peut conna\u00eetre, pardon, qu'on peut conna\u00eetre un mot par la compagnie de ce mot-l\u00e0. Voil\u00e0, donc on va aussi, du coup, mettre en place dans ce m\u00e9canisme de similarit\u00e9 distributionnelle de la similarit\u00e9 entre vecteurs. Et donc, l'hypoth\u00e8se, c'est que le sens d'un mot inconnu, il va \u00eatre devinable par son contexte, et que le contexte va bien s\u00fbr aider \u00e0 caract\u00e9riser le sens du mot. Et puis, la similarit\u00e9 de contexte va aider \u00e0 caract\u00e9riser la similarit\u00e9 de sens. Voil\u00e0, donc \u00e7a, c'est ce qu'on appelle la similarit\u00e9 distributionnelle, qui consiste \u00e0 repr\u00e9senter un mot par ses voisins. Donc typiquement, l'id\u00e9e, c'est \u00e7a. Si je veux caract\u00e9riser banking, eh bien, je veux utiliser les voisins de banking pour caract\u00e9riser justement le mot banking. Et inversement, je peux caract\u00e9riser le contexte d'un mot \u00e0 partir du mot du m\u00eame. \u00c7a, c'est vraiment une des id\u00e9es cl\u00e9s pour les mots de la compagnie. \u00c7a, c'est vraiment une des id\u00e9es cl\u00e9s du traitement du langage naturel moderne, et c'est notamment ce qu'on appelle les techniques d'embedding de mots, type Word2Vec, GloVe, etc., et puis les mod\u00e8les de langues plus r\u00e9cents, pr\u00e9entra\u00een\u00e9es, de type BERT, par exemple. Voil\u00e0, alors la question, c'est comment repr\u00e9senter un mot par ses voisins. Et donc, pour \u00e7a, on va utiliser une repr\u00e9sentation dense de plus petite dimension. L'id\u00e9e, c'est que le nombre de sujets couverts dans un corpus, il est petit, et l'id\u00e9e, \u00e7a va \u00eatre de stocker l'information la plus importante dans ce petit nombre de dimensions. Donc, on retombe sur un probl\u00e8me de r\u00e9duction de la dimension. Alors, comment on peut repr\u00e9senter un mot par ses voisins ? L'approche classique, c'est d'utiliser une matrice de co-occurrence. On l'a parl\u00e9 juste avant. Apr\u00e8s, il y a deux options. Soit je prends en compte tout le document, soit je prends une fen\u00eatre autour de chaque mot. Si je prends en compte tout le document, je vais tomber sur des probl\u00e8mes qui sont connus comme des probl\u00e8mes d'analyse en s\u00e9mantique latente. Et si je prends une fen\u00eatre autour de chaque mot, ben, je suis typiquement dans ce qu'on fait en analyse distributionnelle. Prenons un petit exemple avec une fen\u00eatre de taille A. Donc, j'ai un petit corpus qui est celui-l\u00e0, donc avec un ensemble de termes qui sont I like, deep learning, NLP, enjoy flying, et puis le point. Et puis, comme j'ai une fen\u00eatre de taille 1, ben, je vais construire du coup ma matrice de co-occurrence. Donc, I n'est jamais voisin de I. I est voisin de like deux fois. I like, I like. I est voisin de enjoy une fois. I n'est jamais voisin de deep, etc. Vous avez compris comment on construit cette matrice de co-occurrence avec un voisinage de taille 1. Donc, j'ai comme \u00e7a la repr\u00e9sentation d'un mot par ses voisins, donc avec cette matrice de co-occurrence. Apr\u00e8s, on peut se poser des questions sur cette repr\u00e9sentation, notamment sur son passage \u00e0 l'\u00e9chelle, parce qu'on va avoir une taille de la matrice qui va cro\u00eetre avec le vocabulaire. J'ai une matrice qui est non dense, et donc l'id\u00e9e, \u00e7a va \u00eatre d'essayer de stocker l'information qui est contenue dans cette matrice dans un nombre fix\u00e9 plus petit de dimensions. Notamment, on estime qu'entre 25 et 1000... 1000 dimensions, c'est tr\u00e8s bien. Et donc on va essayer d'utiliser sur cette matrice de co-occurrence des techniques de r\u00e9duction de la dimension. Alors, une technique de r\u00e9duction de la dimension qui est assez ad\u00e9quate pour ce probl\u00e8me, c'est la d\u00e9composition en valeur singuli\u00e8re. On sait que toute matrice A de dimension m fois n, avec m qui est sup\u00e9rieur \u00e0 n, on peut la d\u00e9composer de la mani\u00e8re suivante. A, c'est U, sigma, transpos\u00e9 de V, avec U qui est de dimension m fois m unitaire, telle que U, T, U c'est la matrice identit\u00e9, sigma qui est une matrice m fois n diagonale avec des coefficients r\u00e9els positifs, donc les sigma 1 sup\u00e9rieur \u00e0 sigma 2, etc. Et puis V qui est de dimension n fois n et qui est aussi unitaire. Et puis les sigma i, en fait, ce sont ce qu'on appelle les valeurs singuli\u00e8res de la matrice A. Voil\u00e0, donc une illustration de cette d\u00e9composition en valeur singuli\u00e8re. Pour une matrice X qui est ici de taille V fois V, cardinal de V \u00e9tant la taille de notre vocabulaire, j'ai cette d\u00e9composition avec cette premi\u00e8re matrice U de taille cardinal V fois cette matrice diagonale, avec les sigma 1 qui vont \u00eatre les valeurs singuli\u00e8res, et puis donc la matrice V. Donc les valeurs singuli\u00e8res de X sont les racines carr\u00e9es des valeurs propres de X, T, X. Les colonnes de V, les valeurs singuli\u00e8res droits de X, c'est les vecteurs propres de X, T, X. Et les colonnes de U, c'est les vecteurs propres de X, X, T. Voil\u00e0, cette d\u00e9composition en valeur singuli\u00e8re, on peut l'utiliser pour r\u00e9duire la dimension en s\u00e9lectionnant uniquement les cas premiers vecteurs singuliers. Je me contente ici de s\u00e9lectionner les cas premiers vecteurs singuliers, donc je tronque ma matrice U et ma matrice V. Voil\u00e0, donc la r\u00e9duction de la dimension avec la SVD, c'est \u00e9tant donn\u00e9 un vocabulaire de terme V, je g\u00e9n\u00e8re la matrice X de taille cardinal V fois cardinal V, j'applique la SVD pour obtenir U, S et V, et puis je choisis les cas premi\u00e8res colonnes de U pour avoir des vecteurs de mots de dimension K. Et \u00e7a, du coup, ces cas premi\u00e8res colonnes de U, c'est ce qu'on peut appeler des repr\u00e9sentations distributionnelles de m\u00e9moire. Et puis on a aussi des informations statistiques qui indiquent comment le fait de choisir que les cas premi\u00e8res dimensions capturent ou pas de l'information. Voil\u00e0, reprenons notre petit exemple de tout \u00e0 l'heure et donc appliquons ce principe de r\u00e9duction de la dimension sur ce petit exemple-l\u00e0. Donc j'ai mon ensemble de mots qui est celui-l\u00e0, je fais appel au module linale de NumPy, et puis j'applique la SVD sur cette matrice X qui est donc ma matrice de co-occurrence telle que construite tout \u00e0 l'heure, avec un voisinage de contexte, une fen\u00eatre de contexte de taille 1. Je vais du coup, apr\u00e8s, s\u00e9lectionner les deux premi\u00e8res colonnes de U, donc les deux plus grandes valeurs singuli\u00e8res, ce que je fais avec ce code-l\u00e0, et puis j'affiche du coup les mots obtenus, les repr\u00e9sentations des mots obtenus, donc dans cet espace \u00e0 deux dimensions. Et donc on voit que c'est int\u00e9ressant, que j'ai peut-\u00eatre effectivement capt\u00e9 une information un peu plus s\u00e9mantique avec ce type de repr\u00e9sentation, et notamment le fait que NLP soit proche de DEEP dans cet espace, c'est relativement pertinent. ENJOY et LIKE sont pas tr\u00e8s loin non plus dans cet espace. Donc c'est int\u00e9ressant, mais c'est bien s\u00fbr co\u00fbteux en termes de calcul, et puis il est difficile d'incorporer de nouveaux mots, de nouveaux documents. Juste ce qu'il faut retenir de cette approche, c'est qu'on va souhaiter repr\u00e9senter un mot par un vecteur dense. Voil\u00e0, typiquement ce qu'on souhaite c'est un mot associ\u00e9 \u00e0 ce type de vecteur l\u00e0, de petite dimension. Et donc cette repr\u00e9sentation, en fait on peut la prendre, et notamment on peut la prendre avec des approches qu'utilisent des r\u00e9seaux de neurones profonds. Donc c'est vraiment cette id\u00e9e-l\u00e0, apprendre directement la repr\u00e9sentation dans ce d\u00e9mo, et \u00e7a c'est typiquement ce qui a \u00e9t\u00e9 propos\u00e9 par Mikolov en 2013 avec l'approche Word2Vec, o\u00f9 l'id\u00e9e c'\u00e9tait de pr\u00e9dire le contexte des mots directement plut\u00f4t que de compter les co-occurrences. Donc apr\u00e8s il y a eu une famille \u00e9norme d'algorithmes qui ont suivi, donc GloVe, BERT, qui est des mod\u00e8les de langues un peu plus sophistiqu\u00e9s, mais il y a eu \u00e9norm\u00e9ment d'extensions de cette id\u00e9e. Donc si je reprends Word2Vec, il y a principalement deux algorithmes. L'algorithme qu'on appelle CEBO, donc Continuous Back of Word Model, qui pr\u00e9dit le mot cible \u00e9tant donn\u00e9 son contexte, donc \u00e9tant donn\u00e9 son voisinage, ou l'approche Skipgram qui elle, cherche \u00e0 pr\u00e9dire le contexte \u00e9tant donn\u00e9 le mot cible. Donc on va regarder l'approche Skipgram, donc on a des mod\u00e8les de langues, des mod\u00e8les de mots. Voil\u00e0, donc on a le mot banking qui est le mot central, et puis en fait je vais chercher \u00e0 pr\u00e9dire le contexte, c'est-\u00e0-dire les mots pr\u00e9c\u00e9dents et les mots suivants, justement en essayant d'estimer la probabilit\u00e9 que tel mot soit le mot avant banking, que tel mot soit le mot apr\u00e8s banking, etc. Donc j'ai mon mot central, j'ai donc mon fen\u00eatrage de M mots, et puis l'objectif va \u00eatre de pr\u00e9dire le contexte, c'est-\u00e0-dire les mots voisins dans cette fen\u00eatre de taille M \u00e9tant donn\u00e9 le mot cible. Et donc pour \u00e7a, je vais avoir une fonction objective qui va chercher \u00e0 maximiser la log probabilit\u00e9 de chaque mot de contexte \u00e9tant donn\u00e9 le mot cible. Donc j'ai ces probabilit\u00e9s d'un mot \u00e9tant donn\u00e9 le mot cible, d'un mot de mot contexte, avec M qui est la taille de ma fen\u00eatre, et puis je vais chercher \u00e0 maximiser \u00e7a pour l'ensemble des mots \u00e0 ma disposition, de mon corpus. Et donc ce que je cherche, c'est \u00e0 trouver l'ensemble des variables qui vont me permettre de maximiser \u00e7a, et donc c'est \u00e7a que je cherche \u00e0 optimiser. Donc ce qu'on fait g\u00e9n\u00e9ralement dans ces approches-l\u00e0, c'est qu'on va transformer une valeur qui est dans RV \u00e0 une distribution de probabilit\u00e9 au travers d'une fonction qui s'appelle une fonction softmax, et donc qui est d\u00e9finie de cette mani\u00e8re-l\u00e0. Et donc si j'applique \u00e7a sur ce cadre-l\u00e0, sur notre probl\u00e8me, je vais avoir une fonction softmax et donc une formulation pour notre probl\u00e8me qui va devenir celle-ci. Donc on voit qu'on prend bien en compte la similarit\u00e9 entre nos mots en faisant \u00e7a. Donc O et C ici, c'est les indices de mots de sortie et centraux, et puis U et V, c'est les U de O et V, c'est les vecteurs correspondants. Donc regardons plus en d\u00e9tail cette architecture d'apprentissage de repr\u00e9sentation de mots par des approches neuronales. Donc j'ai la repr\u00e9sentation de mon mot central, ici c'est l'entra\u00eenement syst\u00e8me, donc repr\u00e9sentation de type one-hot. J'ai une premi\u00e8re matrice de taille D x V. D, c'est la taille de mon vocabulaire, c'est l'esp\u00e8ce de repr\u00e9sentation de mes termes en one-hot. Et puis D, c'est la dimension que je souhaite pour mon vecteur dans mes digues. Donc avec des param\u00e8tres bien s\u00fbr que j'initialise, mais que je vais chercher \u00e0 estimer. Et puis en sortie de la multiplication de cette repr\u00e9sentation one-hot de mon mot et de la matrice, j'ai mon vecteur Vc, qui est mon vecteur dans mes digues que je cherche. Ce vecteur-l\u00e0, je vais donc ensuite le remultiplier avec mes mots de contexte. Voil\u00e0, donc \u00e0 nouveau j'ai une matrice de taille D x V, la taille de mon vocabulaire, donc avec ici les mots de mon contexte. J'en sorti le produit scalaire entre mon mot central et son mot de contexte, et je transforme ces valeurs-l\u00e0 en softmax, comme indiqu\u00e9, qui me donne une estimation de la probabilit\u00e9 de x sachant c. Du fait que le mot x soit un mot de contexte de c, et puis que je peux mettre en relation avec ma v\u00e9rit\u00e9. Et donc avec le principe d'optimisation des r\u00e9seaux de neurones, et notamment la descente de gradients, je vais pouvoir petit \u00e0 petit apprendre les diff\u00e9rentes points de ma matrice, et donc je vais pouvoir avoir en sortie la repr\u00e9sentation distributionnelle, donc le plongement, le mot dans mes dignes, de mon mot de contexte, donc ici. Alors bien s\u00fbr, il y a plein d'applications de tout \u00e7a \u00e0 l'AERI, notamment pour construire des repr\u00e9sentations des requ\u00eates et des textes selon ce formalisme, donc avec ces repr\u00e9sentations de mots. L'id\u00e9e c'est d'appliquer \u00e7a \u00e0 chaque mot de la requ\u00eate, chaque mot du texte, et puis apr\u00e8s avec des fonctions d'agr\u00e9gation, on obtient une repr\u00e9sentation de la requ\u00eate et une repr\u00e9sentation du texte qu'on peut apr\u00e8s mettre en correspondance avec tous les mod\u00e8les qu'on a \u00e0 notre disposition. Et puis il y a plein d'autres architectures qui utilisent ces m\u00e9canismes-l\u00e0. Une slide qui montre l'ensemble des choses qui sont possibles, l'ensemble des architectures qui sont possibles et qui sont toutes plus complexes que les unes que les autres. Voil\u00e0, j'en ai fini sur ce cours, donc rapide bilan sur ce qui vous a \u00e9t\u00e9 pr\u00e9sent\u00e9. Donc un nouveau paradigme pour l'AERI au travers de cette approche Learning to Rank. Donc \u00e7a, j'avais d\u00e9j\u00e0 dit la derni\u00e8re fois, des approches qui tentent d'exploiter toutes les informations \u00e0 disposition, des r\u00e9sultats comparables \u00e0 ceux des mod\u00e8les probabilistes dans le cadre de collection classique et des r\u00e9sultats bien meilleurs quand on a des grosses collections pour lesquelles on est capable de construire des espaces d'attributs tr\u00e8s riches. Et puis bien s\u00fbr, ce probl\u00e8me de disponibilit\u00e9 des donn\u00e9es d'entr\u00e9e, des donn\u00e9es \u00e0 noter pour mettre en place les techniques d'apprentissage. Voil\u00e0, des lectures conseill\u00e9es pour compl\u00e9ter ce cours. Et puis je vous souhaite un bon lab qui vous demandera d'appliquer l'approche par paire d'une part et d'autre part, qui vous fera travailler justement sur cette partie d'ouverture, donc sur les repr\u00e9sentations de mots par plongement lexico."
}